{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/17 23:24:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "spark = SparkSession.builder.appName('pandas_UDF_as_decorator').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame([[1, \"a string\", (\"a nested string\",)],\n",
    "                            [3, \"a single word\", (\"a simple sentence\",)]],\n",
    "                     \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+\n",
      "|long_col|   string_col|         struct_col|\n",
      "+--------+-------------+-------------------+\n",
      "|       1|     a string|  {a nested string}|\n",
      "|       3|a single word|{a simple sentence}|\n",
      "+--------+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|new_colum              |\n",
      "+-----------------------+\n",
      "|{a nested string, 9}   |\n",
      "|{a simple sentence, 16}|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\").alias(\"new_colum\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series to Series\n",
    "The function takes one or more pandas.Series and outputs one pandas.Series. The output of the function should always be of the same length as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|upper      |\n",
      "+-----------+\n",
      "|JOHN DOE   |\n",
      "|JUAN GARCIA|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"string\")\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",), (\"Juan Garcia\",)], (\"name\",))\n",
    "df.select(to_upper(\"name\").alias('upper')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      splitted|\n",
      "+--------------+\n",
      "|   {John, Doe}|\n",
      "|{Juan, Garcia}|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"first string, last string\")\n",
    "def split_expand(s: pd.Series) -> pd.DataFrame:\n",
    "    return s.str.split(expand=True)\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",), (\"Juan Garcia Smith\",)], (\"name\",))\n",
    "df.select(split_expand(\"name\").alias('splitted')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator of Series to Iterator of Series\n",
    "The function takes an iterator of pandas.Series and outputs an iterator of pandas.Series. In this case, the created pandas UDF instance requires one input column when this is called as a PySpark column. The length of the entire output from the function should be the same length of the entire input; therefore, it can prefetch the data from the input iterator as long as the lengths are the same.\n",
    "\n",
    "It is also useful when the UDF execution requires initializing some states although internally it works identically as Series to Series case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|plus_one|\n",
      "+--------+\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for s in iterator:\n",
    "        yield s + 1\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.select(plus_one(df.v).alias('plus_one')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator of Multiple Series to Iterator of Series\n",
    "The function takes an iterator of a tuple of multiple pandas.Series and outputs an iterator of pandas.Series. In this case, the created pandas UDF instance requires input columns as many as the series when this is called as a PySpark column. Otherwise, it has the same characteristics and restrictions as Iterator of Series to Iterator of Series case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|  v|output|\n",
      "+---+------+\n",
      "|  1|     1|\n",
      "|  2|     4|\n",
      "|  3|     9|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterator, Tuple\n",
    "from pyspark.sql.functions import struct, col\n",
    "@pandas_udf(\"long\")\n",
    "def multiply(iterator: Iterator[Tuple[pd.Series, pd.DataFrame]]) -> Iterator[pd.Series]:\n",
    "    for s1, df in iterator:\n",
    "        yield s1 * df.v\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.withColumn('output', multiply(col(\"v\"), struct(col(\"v\")))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series to Scalar\n",
    "The function takes pandas.Series and returns a scalar value. The returnType should be a primitive data type, and the returned scalar can be either a python primitive type, e.g., int or float or a numpy data type, e.g., numpy.int64 or numpy.float64. Any should ideally be a specific scalar type accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1| 1.0|\n",
      "|  1| 2.0|\n",
      "|  2| 3.0|\n",
      "|  2| 5.0|\n",
      "|  2|10.0|\n",
      "+---+----+\n",
      "\n",
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame([(1, 1.0), \n",
    "                            (1, 2.0), \n",
    "                            (2, 3.0), \n",
    "                            (2, 5.0), \n",
    "                            (2, 10.0)], (\"id\", \"v\"))\n",
    "df.show()                            \n",
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This UDF can also be used as window functions as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.0|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   3.0|\n",
      "|  2| 5.0|   4.0|\n",
      "|  2|10.0|   7.5|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v: pd.Series) -> float:\n",
    "    return v.mean()\n",
    "\n",
    "df = spark.createDataFrame([(1, 1.0), \n",
    "                            (1, 2.0), \n",
    "                            (2, 3.0), \n",
    "                            (2, 5.0), \n",
    "                            (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "# PARTITION BY 'id' ORDER BY 'v' ROWS BETWEEN start=-1 and end=0 (PRECEDING AND CURRENT ROW) both inclusive \n",
    "# Both start and end are relative positions from the current row                    \n",
    "w = Window.partitionBy('id').orderBy('v').rowsBetween(-1, 0)\n",
    "df.withColumn('mean_v', mean_udf(\"v\").over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split by id:  (1, 1.0), (1, 2.0) , 1 row mean of 1 = 1, 2 row mean of 1 and 2 =  1.5\n",
    "              (2, 3.0), (2, 5.0), (2, 10.0) , 3 row mean of 3 = 3, 4 row mean of 3 and 5 =  4.0, 5 row mean of 5 and 10 = 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/sql/pandas/group_ops.py:102: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-1.0|\n",
      "|  1| 0.0|\n",
      "|  2|-1.0|\n",
      "|  2| 1.0|\n",
      "|  2| 6.0|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "df = spark.createDataFrame([(1, 1.0), \n",
    "                            (1, 2.0), \n",
    "                            (2, 3.0), \n",
    "                            (2, 5.0), \n",
    "                            (2, 10.0)], (\"id\", \"v\"))\n",
    "\n",
    "\n",
    "# wrapping of the functions as a function inside a function\n",
    "# We have defined a normal UDF called 'my_function' that takes the Pyspark DF and the argument to be used in the core pandas groupby. \n",
    "def my_function(df, by=\"id\", column=\"v\", value=1.0):\n",
    "    schema = \"{} long, {} double\".format(by, column)\n",
    "\n",
    "    # schema is used to map the Pandas DF returned by subtract_value (return pdf.assign(v = v - g * value)).\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def subtract_value(pdf):\n",
    "        # pdf is a pandas.DataFrame\n",
    "        v = pdf[column]\n",
    "        g = pdf[by]\n",
    "        return pdf.assign(v = v - g * value)\n",
    "\n",
    "    # function 'subtract_value' defined as a Pandas UDF and it does not have any arguments. \n",
    "    # However, we are still inside the scope of 'my_function' when 'subtract_value' is called\n",
    "    # as 'apply' and it has value defined\n",
    "    return df.groupby(by).apply(subtract_value)\n",
    "\n",
    "my_function(df, by=\"id\", column=\"v\", value=2.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows a Pandas UDF to simply add one to each value, in which it is defined with the function called pandas_plus_one decorated by pandas_udf with the Pandas UDF type specified as PandasUDFType.SCALAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/sql/pandas/functions.py:398: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|pandas_plus_one(id)|\n",
      "+-------------------+\n",
      "|                1.0|\n",
      "|                2.0|\n",
      "|                3.0|\n",
      "|                4.0|\n",
      "|                5.0|\n",
      "|                6.0|\n",
      "|                7.0|\n",
      "|                8.0|\n",
      "|                9.0|\n",
      "|               10.0|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one(v):\n",
    "    # `v` is a pandas Series\n",
    "    return v.add(1)  # outputs a pandas Series\n",
    "\n",
    "spark.range(10).select(pandas_plus_one(\"id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|pandas_plus_one_long(id)|\n",
      "+------------------------+\n",
      "|                       1|\n",
      "|                       2|\n",
      "|                       3|\n",
      "|                       4|\n",
      "|                       5|\n",
      "|                       6|\n",
      "|                       7|\n",
      "|                       8|\n",
      "|                       9|\n",
      "|                      10|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf('long', PandasUDFType.SCALAR)\n",
    "def pandas_plus_one_long(v):\n",
    "    # `v` is a pandas Series\n",
    "    return v.add(1)  # outputs a pandas Series\n",
    "\n",
    "spark.range(10).select(pandas_plus_one_long(\"id\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Pandas APIs with Python Type Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|((pandas_plus_one((id - 1)) + LOG2(id)) + 1)|\n",
      "+--------------------------------------------+\n",
      "|                                        null|\n",
      "|                                         2.0|\n",
      "|                                         4.0|\n",
      "|                           5.584962500721156|\n",
      "|                                         7.0|\n",
      "|                           8.321928094887362|\n",
      "|                           9.584962500721156|\n",
      "|                          10.807354922057604|\n",
      "|                                        12.0|\n",
      "|                          13.169925001442312|\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas UDF\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, log2, col\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "# pandas_plus_one(\"id\") is identically treated as _a SQL expression_ internally.\n",
    "# Namely, you can combine with other columns, functions and expressions.\n",
    "spark.range(10).select(\n",
    "    pandas_plus_one(col(\"id\") - 1) + log2(\"id\") + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas Function API\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pandas_plus_one(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    return map(lambda v: v + 1, iterator)\n",
    "\n",
    "\n",
    "# pandas_plus_one is just a regular Python function, and mapInPandas is\n",
    "# logically treated as _a separate SQL query plan_ instead of a SQL expression. \n",
    "# Therefore, direct interactions with other expressions are impossible.\n",
    "spark.range(10).mapInPandas(pandas_plus_one, schema=\"id long\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
