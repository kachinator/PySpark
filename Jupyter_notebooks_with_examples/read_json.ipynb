{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('readJson').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON file into DataFrame\n",
    "Using `read.json(\"path\")` or `read.format(\"json\").load(\"path\")` you can read a JSON file into a PySpark DataFrame, these methods take a file path as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|  Long|Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22| NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22| NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE|-66.26| NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE|-97.31| NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY|-97.33| NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file into dataframe\n",
    "df = spark.read.json(\"./resources/json_files/zipcodes.json\")\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|  Long|Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22| NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22| NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE|-66.26| NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE|-97.31| NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY|-97.33| NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+------+-----+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format('org.apache.spark.sql.json').load(\"./resources/json_files/zipcodes.json\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON file from multiline\n",
    "PySpark JSON data source provides multiple options to read files in different options, use multiline option to read JSON files scattered across multiple lines. By default multiline option, is set to false.\n",
    "\n",
    "\n",
    "Using read.option(\"multiline\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multiline json file\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"./resources/json_files/multiline-zipcode.json\")\n",
    "multiline_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple files at a time\n",
    "Using the read.json() method you can also read multiple JSON files from different paths, just pass all file names with fully qualified paths by separating comma, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+--------------------+---------------+\n",
      "|     _corrupt_record|     action|          customer|              device|           time|\n",
      "+--------------------+-----------+------------------+--------------------+---------------+\n",
      "|[{\"time\":\"7:15:38...|       NULL|              NULL|                NULL|           NULL|\n",
      "|                NULL|   power on| Pietro MacMenamie|         Amazon Echo|10:17:17.000 AM|\n",
      "|                NULL|   power on|  Diego Caudrelier|  GreenIQ Controller|10:19:17.000 AM|\n",
      "|                NULL|low battery|       Ben Humpage|         Amazon Echo| 7:07:41.000 AM|\n",
      "|                NULL|low battery|    Stillman Tatum|Nest T3021US Ther...| 2:07:18.000 PM|\n",
      "|                NULL|   power on|   Carolann Fernez|         Amazon Echo| 7:33:40.000 AM|\n",
      "|                NULL|  power off|      Gran Torbeck| August Doorbell Cam|11:30:10.000 AM|\n",
      "|                NULL|low battery|Therine Jakubowski|         Amazon Echo| 4:52:14.000 PM|\n",
      "|                NULL|  power off|  Olympia Crumpton|         Amazon Echo| 2:43:19.000 PM|\n",
      "|                NULL|   power on|   Murry Benedetti|             ecobee4|12:21:59.000 PM|\n",
      "|[{\"time\":\"3:57:09...|       NULL|              NULL|                NULL|           NULL|\n",
      "|                NULL|  power off| Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|\n",
      "|                NULL|   power on|    Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|\n",
      "|                NULL|  power off|  Kippar McCaughen|             ecobee4| 5:40:19.000 PM|\n",
      "|                NULL|  power off|     Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|\n",
      "|                NULL|  power off|     Fanya Menzies|             ecobee4| 3:12:48.000 PM|\n",
      "|                NULL|low battery|     Jeanne Gresch|             ecobee4| 5:39:47.000 PM|\n",
      "|                NULL|   power on|     Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|\n",
      "|                NULL|  power off|        Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|\n",
      "|                NULL|  power off|  Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|\n",
      "+--------------------+-----------+------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multiple files\n",
    "df2 = spark.read.json(\n",
    "    ['./resources/json_files/n1.json','./resources/json_files/n2.json'])\n",
    "df2.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all files in a directory\n",
    "We can read all JSON files from a directory into DataFrame just by passing directory as a path to the json() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 17:27:10 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: ./resources/json_files/n*.json.\n",
      "java.io.FileNotFoundException: File resources/json_files/n*.json does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+--------------------+---------------+\n",
      "|     _corrupt_record|     action|          customer|              device|           time|\n",
      "+--------------------+-----------+------------------+--------------------+---------------+\n",
      "|[{\"time\":\"7:15:38...|       NULL|              NULL|                NULL|           NULL|\n",
      "|                NULL|   power on| Pietro MacMenamie|         Amazon Echo|10:17:17.000 AM|\n",
      "|                NULL|   power on|  Diego Caudrelier|  GreenIQ Controller|10:19:17.000 AM|\n",
      "|                NULL|low battery|       Ben Humpage|         Amazon Echo| 7:07:41.000 AM|\n",
      "|                NULL|low battery|    Stillman Tatum|Nest T3021US Ther...| 2:07:18.000 PM|\n",
      "|                NULL|   power on|   Carolann Fernez|         Amazon Echo| 7:33:40.000 AM|\n",
      "|                NULL|  power off|      Gran Torbeck| August Doorbell Cam|11:30:10.000 AM|\n",
      "|                NULL|low battery|Therine Jakubowski|         Amazon Echo| 4:52:14.000 PM|\n",
      "|                NULL|  power off|  Olympia Crumpton|         Amazon Echo| 2:43:19.000 PM|\n",
      "|                NULL|   power on|   Murry Benedetti|             ecobee4|12:21:59.000 PM|\n",
      "|[{\"time\":\"3:57:09...|       NULL|              NULL|                NULL|           NULL|\n",
      "|                NULL|  power off| Nicolle Pargetter| August Doorbell Cam| 1:29:05.000 AM|\n",
      "|                NULL|   power on|    Concordia Muck|Footbot Air Quali...| 6:02:06.000 AM|\n",
      "|                NULL|  power off|  Kippar McCaughen|             ecobee4| 5:40:19.000 PM|\n",
      "|                NULL|  power off|     Sidney Jotham|  GreenIQ Controller| 4:54:28.000 PM|\n",
      "|                NULL|  power off|     Fanya Menzies|             ecobee4| 3:12:48.000 PM|\n",
      "|                NULL|low battery|     Jeanne Gresch|             ecobee4| 5:39:47.000 PM|\n",
      "|                NULL|   power on|     Chen Cuttelar| August Doorbell Cam| 2:45:44.000 PM|\n",
      "|                NULL|  power off|        Merwyn Mix|         Amazon Echo| 9:23:41.000 PM|\n",
      "|                NULL|  power off|  Angelico Conrath|         Amazon Echo| 4:53:13.000 AM|\n",
      "+--------------------+-----------+------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all JSON files from a folder\n",
    "df3 = spark.read.json(\"./resources/json_files/n*.json\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files with a user-specified custom schema\n",
    "PySpark Schema defines the structure of the data, in other words, it is the structure of the DataFrame. PySpark SQL provides StructType & StructField classes to programmatically specify the structure to the DataFrame.\n",
    "\n",
    "If you know the schema of the file ahead and do not want to use the default inferSchema option, use schema option to specify user-defined custom column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: integer (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|   Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|        Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|     Parc Parque, PR|NA-US-PR-PARC PARQUE|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72| -97.31| -0.1|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       61392|  76177|   STANDARD|         FORT WORTH|   TX|       PRIMARY|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|      Fort Worth, TX| NA-US-TX-FORT WORTH|        false|           2126|               4053| 122396986|         NULL|\n",
      "|       61393|  76177|   STANDARD|           FT WORTH|   TX|    ACCEPTABLE|32.75| -97.33| -0.1|-0.83| 0.54|         NA|     US|        Ft Worth, TX|   NA-US-TX-FT WORTH|        false|           2126|               4053| 122396986|         NULL|\n",
      "|           4|    704|   STANDARD|    URB EUGENE RICE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US| Urb Eugene Rice, PR|NA-US-PR-URB EUGE...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       39827|  85209|   STANDARD|               MESA|   AZ|       PRIMARY|33.37|-111.64| -0.3|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14962|              26883| 563792730|no NWS data, |\n",
      "|       39828|  85210|   STANDARD|               MESA|   AZ|       PRIMARY|33.38|-111.84|-0.31|-0.77| 0.55|         NA|     US|            Mesa, AZ|       NA-US-AZ-MESA|        false|          14374|              25446| 471000465|         NULL|\n",
      "|       49345|  32046|   STANDARD|           HILLIARD|   FL|       PRIMARY|30.69| -81.92| 0.12|-0.85| 0.51|         NA|     US|        Hilliard, FL|   NA-US-FL-HILLIARD|        false|           3922|               7443| 133112149|         NULL|\n",
      "|       49346|  34445|     PO BOX|             HOLDER|   FL|       PRIMARY|28.96| -82.41| 0.11|-0.86| 0.48|         NA|     US|          Holder, FL|     NA-US-FL-HOLDER|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       49347|  32564|   STANDARD|               HOLT|   FL|       PRIMARY|30.72| -86.67| 0.04|-0.85| 0.51|         NA|     US|            Holt, FL|       NA-US-FL-HOLT|        false|           1207|               2190|  36395913|         NULL|\n",
      "|       49348|  34487|     PO BOX|          HOMOSASSA|   FL|       PRIMARY|28.78| -82.61| 0.11|-0.86| 0.48|         NA|     US|       Homosassa, FL|  NA-US-FL-HOMOSASSA|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|          10|    708|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14| -66.26| 0.38|-0.86| 0.31|         NA|     US|    Bda San Luis, PR|NA-US-PR-BDA SAN ...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|           3|    704|   STANDARD|      SECT LANAUSSE|   PR|NOT ACCEPTABLE|17.96| -66.22| 0.38|-0.87|  0.3|         NA|     US|   Sect Lanausse, PR|NA-US-PR-SECT LAN...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54354|  36275|     PO BOX|      SPRING GARDEN|   AL|       PRIMARY|33.97| -85.55| 0.06|-0.82| 0.55|         NA|     US|   Spring Garden, AL|NA-US-AL-SPRING G...|        false|           NULL|               NULL|      NULL|         NULL|\n",
      "|       54355|  35146|   STANDARD|        SPRINGVILLE|   AL|       PRIMARY|33.77| -86.47| 0.05|-0.82| 0.55|         NA|     US|     Springville, AL|NA-US-AL-SPRINGVILLE|        false|           4046|               7845| 172127599|         NULL|\n",
      "|       54356|  35585|   STANDARD|        SPRUCE PINE|   AL|       PRIMARY|34.37| -87.69| 0.03|-0.82| 0.56|         NA|     US|     Spruce Pine, AL|NA-US-AL-SPRUCE PINE|        false|            610|               1209|  18525517|         NULL|\n",
      "|       76511|  27007|   STANDARD|           ASH HILL|   NC|NOT ACCEPTABLE| 36.4| -80.56| 0.13|-0.79| 0.59|         NA|     US|        Ash Hill, NC|   NA-US-NC-ASH HILL|        false|            842|               1666|  28876493|         NULL|\n",
      "|       76512|  27203|   STANDARD|           ASHEBORO|   NC|       PRIMARY|35.71| -79.81| 0.14|-0.79| 0.58|         NA|     US|        Asheboro, NC|   NA-US-NC-ASHEBORO|        false|           8355|              15228| 215474318|         NULL|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+-------+-----+-----+-----+-----------+-------+--------------------+--------------------+-------------+---------------+-------------------+----------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, DoubleType, StringType, StructType, StructField\n",
    "\n",
    "\n",
    "# Define custom schema\n",
    "schema = StructType([\n",
    "      StructField(\"RecordNumber\",IntegerType(),True),\n",
    "      StructField(\"Zipcode\",IntegerType(),True),\n",
    "      StructField(\"ZipCodeType\",StringType(),True),\n",
    "      StructField(\"City\",StringType(),True),\n",
    "      StructField(\"State\",StringType(),True),\n",
    "      StructField(\"LocationType\",StringType(),True),\n",
    "      StructField(\"Lat\",DoubleType(),True),\n",
    "      StructField(\"Long\",DoubleType(),True),\n",
    "      StructField(\"Xaxis\",DoubleType(),True),\n",
    "      StructField(\"Yaxis\",DoubleType(),True),\n",
    "      StructField(\"Zaxis\",DoubleType(),True),\n",
    "      StructField(\"WorldRegion\",StringType(),True),\n",
    "      StructField(\"Country\",StringType(),True),\n",
    "      StructField(\"LocationText\",StringType(),True),\n",
    "      StructField(\"Location\",StringType(),True),\n",
    "      StructField(\"Decommisioned\",BooleanType(),True),\n",
    "      StructField(\"TaxReturnsFiled\",IntegerType(),True),\n",
    "      StructField(\"EstimatedPopulation\",IntegerType(),True),\n",
    "      StructField(\"TotalWages\",IntegerType(),True),\n",
    "      StructField(\"Notes\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema).json(\"./resources/json_files/zipcodes.json\")\n",
    "# df_with_schema = spark.read.json(\"./resources/json_files/zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Read JSON file using PySpark SQL\n",
    "PySpark SQL also provides a way to read a JSON file by creating a temporary view directly from the reading file using spark.sqlContext.sql(“load JSON to temporary view”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               NULL|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         NULL|           1|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               NULL|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         NULL|           2|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               NULL|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         NULL|       61391|   TX|           NULL|      NULL|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         NULL|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         NULL|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               NULL|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         NULL|           4|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         NULL|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         NULL|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               NULL|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         NULL|       49346|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         NULL|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               NULL|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         NULL|       49348|   FL|           NULL|      NULL|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               NULL|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         NULL|          10|   PR|           NULL|      NULL|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               NULL|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         NULL|           3|   PR|           NULL|      NULL|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               NULL|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         NULL|       54354|   AL|           NULL|      NULL|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         NULL|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         NULL|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         NULL|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         NULL|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE OR REPLACE TEMPORARY VIEW zipcode USING json OPTIONS\" + \" (path './resources/json_files/zipcodes.json')\")\n",
    "spark.sql(\"select * from zipcode\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Saving modes\n",
    "PySpark DataFrameWriter also has a method mode() to specify SaveMode; the argument to this method either takes overwrite, append, ignore, errorifexists.\n",
    "\n",
    "* overwrite – mode is used to overwrite the existing file\n",
    "* append – To add the data to the existing file\n",
    "* ignore – Ignores write operation when the file already exists\n",
    "* errorifexists or error – This is a default option when the file already exists, it returns an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multiline json file\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"./resources/json_files/multiline-zipcode.json\")\n",
    "\n",
    "multiline_df.show() \n",
    "\n",
    "multiline_df.write.mode('Overwrite').json(\"./resources/json_files/writenFile.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
