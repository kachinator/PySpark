{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e654d9fb",
   "metadata": {},
   "source": [
    "## JSON Functions\n",
    "from_json() – Converts JSON string into Struct type or Map type.\n",
    "\n",
    "to_json() – Converts MapType or Struct type to JSON string.\n",
    "\n",
    "json_tuple() – Extract the Data from JSON and create them as a new columns.\n",
    "\n",
    "get_json_object() – Extracts JSON element from a JSON string based on json path specified.\n",
    "\n",
    "schema_of_json() – Create schema string from JSON string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845343ff",
   "metadata": {},
   "source": [
    "### Create DataFrame with Column contains JSON String\n",
    "let’s create DataFrame with a column contains JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740b8635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/17 21:45:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
    "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925f6e92",
   "metadata": {},
   "source": [
    "### from_json()\n",
    "from_json() function is used to convert JSON string into Struct type or Map type. The below example converts JSON string to Map key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed38f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- value: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|id |value                                                                      |\n",
      "+---+---------------------------------------------------------------------------+\n",
      "|1  |[Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR]|\n",
      "+---+---------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert JSON string column to Map type\n",
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8bf93",
   "metadata": {},
   "source": [
    "### to_json()\n",
    "to_json() function is used to convert DataFrame columns MapType or Struct type to JSON string. Using df2 that created from above from_json() example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041e0f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------+\n",
      "|id |value                                                                       |\n",
      "+---+----------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json,col\n",
    "df2.withColumn(\"value\",to_json(col(\"value\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf68b9d",
   "metadata": {},
   "source": [
    "### json_tuple()\n",
    " json_tuple() is used the query or extract the elements from JSON column and create the result as a new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db7c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m----- df\u001b[0m\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[31m----- df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")).toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\")\u001b[0m\n",
      "+---+-------+-----------+-----------+\n",
      "|id |Zipcode|ZipCodeType|City       |\n",
      "+---+-------+-----------+-----------+\n",
      "|1  |704    |STANDARD   |PARC PARQUE|\n",
      "+---+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "from termcolor import cprint\n",
    "cprint(\"----- df\", \"blue\")\n",
    "df.show(truncate=False)\n",
    "cprint('----- df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")).toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\")', \"red\")\n",
    "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
    "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2062cd",
   "metadata": {},
   "source": [
    "### get_json_object()\n",
    "get_json_object() is used to extract the JSON string based on path from the JSON column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5913d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m----- df\u001b[0m\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|id |value                                                                     |\n",
      "+---+--------------------------------------------------------------------------+\n",
      "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
      "+---+--------------------------------------------------------------------------+\n",
      "\n",
      "\u001b[31m----- df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\"))\u001b[0m\n",
      "+---+-----------+\n",
      "|id |ZipCodeType|\n",
      "+---+-----------+\n",
      "|1  |STANDARD   |\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "from termcolor import cprint\n",
    "cprint(\"----- df\", \"blue\")\n",
    "df.show(truncate=False)\n",
    "cprint('----- df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\"))', \"red\")\n",
    "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736ceac",
   "metadata": {},
   "source": [
    "### schema_of_json()\n",
    "Use schema_of_json() to create schema string from JSON string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "806c13b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct<City:string,State:string,ZipCodeType:string,Zipcode:bigint>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import schema_of_json,lit\n",
    "schemaStr=spark.range(1) \\\n",
    "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
    "    .collect()[0][0]\n",
    "print(schemaStr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81edef",
   "metadata": {},
   "source": [
    "### Read and flatten JSON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5545f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|           addresses|contact_type|                  id|\n",
      "+--------------------+------------+--------------------+\n",
      "|[{HILLSBORO, null...|     Company|da38f109-8b8d-d4a...|\n",
      "|[{TAMPA, US, 1284...|  individual|f109da38-d4af-576...|\n",
      "+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = spark.read.option(\"multiline\",\"true\").json(\"./resources/json_files/json4flatten.json\")\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330f545",
   "metadata": {},
   "source": [
    "#### use explode to get addresses' sub-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e45d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "|contact_type|                  id|     city|country_code|               line1|postal_code|state_code|\n",
      "+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "|     Company|da38f109-8b8d-d4a...|HILLSBORO|        null|     5210 16TH AVE S|      33619|        FL|\n",
      "|     Company|da38f109-8b8d-d4a...|PLACENTIA|        null|3417 SANDY PORTER RD|      28273|        NC|\n",
      "|     Company|da38f109-8b8d-d4a...|FAIRBANKS|         USA| 455 3RD AVE STE 120|      99701|        AK|\n",
      "|  individual|f109da38-d4af-576...|    TAMPA|          US|     12840 SW RIV RD|      97123|        OR|\n",
      "|  individual|f109da38-d4af-576...|PLACENTIA|        null|1760 PEACHTREE ST...|      30309|        GA|\n",
      "|  individual|f109da38-d4af-576...|  ATLANTA|         USA|         107 POLK ST|      92870|        CA|\n",
      "+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data2_df = data_df.select(\n",
    "    \"contact_type\",\n",
    "    \"id\",\n",
    "    explode(\"addresses\").alias(\"addressesExplode\")\n",
    ").select(\"contact_type\",\"id\", \"addressesExplode.*\")\n",
    "\n",
    "data2_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74e663",
   "metadata": {},
   "source": [
    "restart spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d287f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9ec065be-c57b-45c7-a1f0-2a518fa1ee2a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.1 in central\n",
      "\tfound io.delta#delta-storage;2.1.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 285ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.1 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.1 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9ec065be-c57b-45c7-a1f0-2a518fa1ee2a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/8ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/17 22:20:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "builder = SparkSession.builder.appName(\"delta-table\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9ead81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|index|contacts                                                                                                                                                                                                                                                                                                            |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1    |[{\"contact_type\":'Company',\"id\":\"da38f109-8b8d-d4af-576b-64a0fd87d247\",\"addresses\":[{\"city\":\"HILLSBORO\",\"country_code\":\"US\",\"line1\":\"12840 SW RIV RD\",\"postal_code\":\"97123\",\"state_code\":\"OR\"},{\"city\":\"ATLANTA\",\"country_code\":\"USA\",\"line1\":\"1760 PEACHTREE ST NW 100\",\"postal_code\":\"30309\",\"state_code\":\"GA\"}]}]|\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString=\"\"\"[{\"contact_type\":'Company',\"id\":\"da38f109-8b8d-d4af-576b-64a0fd87d247\",\"addresses\":[{\"city\":\"HILLSBORO\",\"country_code\":\"US\",\"line1\":\"12840 SW RIV RD\",\"postal_code\":\"97123\",\"state_code\":\"OR\"},{\"city\":\"ATLANTA\",\"country_code\":\"USA\",\"line1\":\"1760 PEACHTREE ST NW 100\",\"postal_code\":\"30309\",\"state_code\":\"GA\"}]}]\"\"\"\n",
    "# jsonString=\"\"\"[{\"contact_type\":\"Company\",\"id\":\"da38f109-8b8d-d4af-576b-64a0fd87d247\",\"addresses\":[{\"city\":\"HILLSBORO\",\"country_code\":\"US\",\"line1\":\"12840 SW RIV RD\",\"postal_code\":\"97123\",\"state_code\":\"OR\"}]}]\"\"\"\n",
    "\n",
    "df_x=spark.createDataFrame([(1, jsonString)],[\"index\",\"contacts\"])\n",
    "df_x.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ec7824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- addresses: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- country_code: string (nullable = true)\n",
      " |    |    |-- line1: string (nullable = true)\n",
      " |    |    |-- postal_code: string (nullable = true)\n",
      " |    |    |-- state_code: string (nullable = true)\n",
      " |-- contact_type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_from_df = spark.read.json(df_x.rdd.map(lambda row: row[\"contacts\"]))\n",
    "schema_from_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56c5987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|index|contacts                                                                                                                                             |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1    |[{[{HILLSBORO, US, 12840 SW RIV RD, 97123, OR}, {ATLANTA, USA, 1760 PEACHTREE ST NW 100, 30309, GA}], Company, da38f109-8b8d-d4af-576b-64a0fd87d247}]|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema_relation = schema_from_df.schema\n",
    "df_y = df_x.withColumn('contacts', from_json(col('contacts'), ArrayType(schema_relation)))\n",
    "df_y.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53dc5012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+--------------------+\n",
      "|index|           addresses|contact_type|                  id|\n",
      "+-----+--------------------+------------+--------------------+\n",
      "|    1|[{HILLSBORO, US, ...|     Company|da38f109-8b8d-d4a...|\n",
      "+-----+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2_df = df_y.select(\n",
    "    \"index\",\n",
    "    explode(\"contacts\").alias(\"contactsExplode\")\n",
    ").select(\"index\", \"contactsExplode.*\")\n",
    "\n",
    "data2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ecb5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "|index|contact_type|                  id|     city|country_code|               line1|postal_code|state_code|\n",
      "+-----+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "|    1|     Company|da38f109-8b8d-d4a...|HILLSBORO|          US|     12840 SW RIV RD|      97123|        OR|\n",
      "|    1|     Company|da38f109-8b8d-d4a...|  ATLANTA|         USA|1760 PEACHTREE ST...|      30309|        GA|\n",
      "+-----+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3_df = data2_df.select(\n",
    "    \"index\",\n",
    "    \"contact_type\",\n",
    "    \"id\",\n",
    "    explode(\"addresses\").alias(\"addressesExplode\")\n",
    ").select(\"index\",\"contact_type\",\"id\", \"addressesExplode.*\")\n",
    "\n",
    "data3_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26f87c",
   "metadata": {},
   "source": [
    "### Flatten function\n",
    "check the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5160043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- addresses: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- country_code: string (nullable = true)\n",
      " |    |    |-- line1: string (nullable = true)\n",
      " |    |    |-- postal_code: string (nullable = true)\n",
      " |    |    |-- state_code: string (nullable = true)\n",
      " |-- contact_type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[StructField('index', LongType(), True),\n",
       " StructField('addresses', ArrayType(StructType([StructField('city', StringType(), True), StructField('country_code', StringType(), True), StructField('line1', StringType(), True), StructField('postal_code', StringType(), True), StructField('state_code', StringType(), True)]), True), True),\n",
       " StructField('contact_type', StringType(), True),\n",
       " StructField('id', StringType(), True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data2_df\n",
    "df.printSchema()\n",
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd19d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+--------------------+\n",
      "|index|           addresses|contact_type|                  id|\n",
      "+-----+--------------------+------------+--------------------+\n",
      "|    1|[{HILLSBORO, US, ...|     Company|da38f109-8b8d-d4a...|\n",
      "+-----+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e1928",
   "metadata": {},
   "source": [
    "1- get in a dictionary all elements in the schema that are ArrayType or Structype (others won't need to be flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e366d156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addresses': ArrayType(StructType([StructField('city', StringType(), True), StructField('country_code', StringType(), True), StructField('line1', StringType(), True), StructField('postal_code', StringType(), True), StructField('state_code', StringType(), True)]), True)}\n"
     ]
    }
   ],
   "source": [
    "complex_fields = dict([(field.name, field.dataType) for field in df.schema.fields if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)])\n",
    "print(complex_fields) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96aec7c",
   "metadata": {},
   "source": [
    "2- get all to be flatten col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a37d8bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addresses_\n"
     ]
    }
   ],
   "source": [
    "qualify = list(complex_fields.keys())[0] + \"_\"\n",
    "print(qualify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805b7bc",
   "metadata": {},
   "source": [
    "`explode` creates a row for each element in the array or map column by ignoring null or empty values in array whereas `explode_outer` returns all values in array or map including null or empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c4a6725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- addresses ---\n",
      "instance ArrayType\n",
      "+-----+--------------------+------------+--------------------+\n",
      "|index|           addresses|contact_type|                  id|\n",
      "+-----+--------------------+------------+--------------------+\n",
      "|    1|{HILLSBORO, US, 1...|     Company|da38f109-8b8d-d4a...|\n",
      "|    1|{ATLANTA, USA, 17...|     Company|da38f109-8b8d-d4a...|\n",
      "+-----+--------------------+------------+--------------------+\n",
      "\n",
      "======== StructType([StructField('city', StringType(), True), StructField('country_code', StringType(), True), StructField('line1', StringType(), True), StructField('postal_code', StringType(), True), StructField('state_code', StringType(), True)]) ==\n",
      "=========\n",
      "---- addresses ---\n",
      "instance StructType\n",
      "expanded [Column<'addresses.city AS addresses_city'>, Column<'addresses.country_code AS addresses_country_code'>, Column<'addresses.line1 AS addresses_line1'>, Column<'addresses.postal_code AS addresses_postal_code'>, Column<'addresses.state_code AS addresses_state_code'>]\n",
      "+-----+------------+--------------------+--------------+----------------------+--------------------+---------------------+--------------------+\n",
      "|index|contact_type|                  id|addresses_city|addresses_country_code|     addresses_line1|addresses_postal_code|addresses_state_code|\n",
      "+-----+------------+--------------------+--------------+----------------------+--------------------+---------------------+--------------------+\n",
      "|    1|     Company|da38f109-8b8d-d4a...|     HILLSBORO|                    US|     12840 SW RIV RD|                97123|                  OR|\n",
      "|    1|     Company|da38f109-8b8d-d4a...|       ATLANTA|                   USA|1760 PEACHTREE ST...|                30309|                  GA|\n",
      "+-----+------------+--------------------+--------------+----------------------+--------------------+---------------------+--------------------+\n",
      "\n",
      "======== None ==\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "from termcolor import cprint\n",
    "\n",
    "while len(complex_fields) != 0:\n",
    "    col_name = list(complex_fields.keys())[0]\n",
    "    cprint(f\"---- {col_name} ---\", 'green')\n",
    "\n",
    "    if isinstance(complex_fields[col_name], StructType):\n",
    "        cprint(\"instance StructType\", 'cyan')\n",
    "        if (type =='standard'):\n",
    "            expanded = [col(col_name + '.' + k).alias(k) \n",
    "                    for k in [n.name for n in complex_fields[col_name]]\n",
    "                    ]\n",
    "            cprint(f\"expanded {expanded}\", 'blue')\n",
    "        else:\n",
    "            expanded = [col(col_name + '.' + k).alias(col_name + '_' + k) \n",
    "                    for k in [n.name for n in complex_fields[col_name]]\n",
    "                    ]\n",
    "            cprint(f\"expanded {expanded}\", 'red')\n",
    "\n",
    "        df = df.select(\"*\", *expanded).drop(col_name)\n",
    "        df.show()\n",
    "\n",
    "    elif isinstance(complex_fields[col_name], ArrayType):\n",
    "        cprint(\"instance ArrayType\", 'magenta')\n",
    "        df = df.withColumn(col_name, explode_outer(col_name))\n",
    "        df.show()\n",
    "    \n",
    "    elif (type(complex_fields[col_name]) == ArrayType):\n",
    "        cprint(\"type ArrayType\", 'yellow')\n",
    "        df = df.withColumn(col_name, explode_outer(col_name))\n",
    "        df.show()\n",
    "\n",
    "    complex_fields = dict([\n",
    "        (field.name, field.dataType)\n",
    "        for field in df.schema.fields\n",
    "        if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\n",
    "    ])\n",
    "\n",
    "    cprint(f\"======== {complex_fields.get(col_name)} ==\", \"yellow\")    \n",
    "    cprint(f\"=========\", \"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c209cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "|index|contact_type|                  id|     city|country_code|               line1|postal_code|state_code|\n",
      "+-----+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "|    1|     Company|da38f109-8b8d-d4a...|HILLSBORO|          US|     12840 SW RIV RD|      97123|        OR|\n",
      "|    1|     Company|da38f109-8b8d-d4a...|  ATLANTA|         USA|1760 PEACHTREE ST...|      30309|        GA|\n",
      "+-----+------------+--------------------+---------+------------+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df_col_name in df.columns:\n",
    "    df = df.withColumnRenamed(df_col_name, df_col_name.replace(qualify, \"\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6b304",
   "metadata": {},
   "source": [
    "#### Check how the loop creates and alias the col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "360fe3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<'addresses.city AS city'>, Column<'addresses.country_code AS country_code'>, Column<'addresses.line1 AS line1'>, Column<'addresses.postal_code AS postal_code'>, Column<'addresses.state_code AS state_code'>]\n"
     ]
    }
   ],
   "source": [
    "test_dict = StructType([StructField('city',StringType(),True),\n",
    "                        StructField('country_code',StringType(),True),\n",
    "                        StructField('line1',StringType(),True),\n",
    "                        StructField('postal_code',StringType(),True),\n",
    "                        StructField('state_code',StringType(),True)])            \n",
    "expanded = [col('addresses'  + '.' + k).alias(k) \n",
    "                    for k in [n.name for n in test_dict]]\n",
    "\n",
    "print(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "478f164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  1|  10|\n",
      "|  2|  20|\n",
      "|  3|null|\n",
      "|  4|  40|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(id=1, data=10), Row(id=2, data=20),Row(id=3, data=None),Row(id=4, data=40)])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea3fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|data| id|\n",
      "+----+---+\n",
      "|  10|  1|\n",
      "|  20|  2|\n",
      "|null|  3|\n",
      "|  40|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd = df.toJSON()\n",
    "\n",
    "df_x = spark.read.option('multiline', 'true').json(dd)\n",
    "df_x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b230d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|data|\n",
      "+---+----+\n",
      "|  2|  20|\n",
      "|  3|null|\n",
      "|  1|  10|\n",
      "|  4|  40|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub = df.subtract(df_x)\n",
    "df_sub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "441ff562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|data| id|\n",
      "+----+---+\n",
      "|null|  3|\n",
      "|  20|  2|\n",
      "|  10|  1|\n",
      "|  40|  4|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub = df_x.subtract(df)\n",
    "df_sub.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6e3f05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|a                    |\n",
      "+---------------------+\n",
      "|[{1, 2}, {10, 20}]   |\n",
      "|[{5, null}, {50, 60}]|\n",
      "|[{7, 8}, {0, null}]  |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.createDataFrame([Row(a=[Row(b=1, c=2), Row(b=10, c=20)]), Row(a=[Row(b=5, c=None), Row(b=50, c=60)]), Row(a=[Row(b=7, c=8), Row(b=0, c=None)])])\n",
    "df_2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90f1d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|a                    |\n",
      "+---------------------+\n",
      "|[{1, 2}, {10, 20}]   |\n",
      "|[{5, null}, {50, 60}]|\n",
      "|[{7, 8}, {0, null}]  |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dd_2 = df_2.toJSON()\n",
    "\n",
    "df_x2 = spark.read.option('multiline', 'true').json(dd_2)\n",
    "df_x2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2e7c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub2 = df_2.subtract(df_x2)\n",
    "df_sub2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "777ed31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|a                  |\n",
      "+-------------------+\n",
      "|[{1, 2}, {10, 20}] |\n",
      "|[{5, 9}, {50, 60}] |\n",
      "|[{7, 8}, {0, null}]|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3 = spark.createDataFrame([Row(a=[Row(b=1, c=2), Row(b=10, c=20)]), Row(a=[Row(b=5, c=9), Row(b=50, c=60)]), Row(a=[Row(b=7, c=8), Row(b=0, c=None)])])\n",
    "df_3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70376441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                 a|\n",
      "+------------------+\n",
      "|[{5, 9}, {50, 60}]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sub_x = df_3.subtract(df_2)\n",
    "df_sub_x.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
