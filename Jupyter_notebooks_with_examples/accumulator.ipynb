{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulator\n",
    "`Accumulator` is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
    "\n",
    "`Accumulators` are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property.\n",
    "\n",
    "Using `accumulator()` from SparkContext class we can create an Accumulator in PySpark programming. Users can also create Accumulators for custom types using `AccumulatorParam` class of PySpark.\n",
    "\n",
    "* `sparkContext.accumulator()` is used to define accumulator variables.\n",
    "* `add()` function is used to add/update a value in accumulator\n",
    "* `value` property on the accumulator variable is used to retrieve the value from the accumulator.\n",
    "\n",
    "We can create Accumulators in PySpark for primitive types **int** and **float**. Users can also create Accumulators for custom types using `AccumulatorParam` class of PySpark.\n",
    "\n",
    "### Creating Accumulator Variable\n",
    "create an accumulator variable `accum` of type int and using it to sum all values in an RDD.\n",
    "\n",
    "rdd.foreach() is executed on workers and accum.value is called from PySpark driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/07 22:20:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
    "\n",
    "# create accumulator variable accum using spark.sparkContext.accumulator(0),\n",
    "# with initial value 0\n",
    "accum = spark.sparkContext.accumulator(0)\n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "\n",
    "# iterating each element in an rdd using foreach() action\n",
    "# and adding each element of rdd to accum variable\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling a function\n",
    "rdd still holds the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "accuSum = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "    \n",
    "rdd.foreach(countFun)\n",
    "\n",
    "print(accuSum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10, 11])\n",
    "\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "\n",
    "print(accumCount.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
