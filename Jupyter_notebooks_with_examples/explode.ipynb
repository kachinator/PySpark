{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explode nested array into rows\n",
    "PySpark explode function can be used to explode an Array of Array (nested Array) ArrayType(ArrayType(StringType)) columns to rows on PySpark DataFrame using python\n",
    "\n",
    "Let’s create a DataFrame with a nested array column. From below example column “subjects” is an array of ArraType which holds subjects learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/09 21:07:42 WARN Utils: Your hostname, javier-ubuntu, resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "25/08/09 21:07:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/09 21:07:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/09 21:07:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/09 21:07:43 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/09 21:07:43 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/08/09 21:07:43 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subjects: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------+\n",
      "|name   |subjects                           |\n",
      "+-------+-----------------------------------+\n",
      "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
      "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
      "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
      "+-------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('explode').getOrCreate()\n",
    "\n",
    "arrayArrayData = [\n",
    "  (\"James\",[[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Michael\",[[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]]),\n",
    "  (\"Robert\",[[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"]])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayArrayData, schema = ['name','subjects'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s explode “subjects” array column to array rows. after exploding, it creates a new column ‘col’ with rows represents an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|name   |col               |\n",
      "+-------+------------------+\n",
      "|James  |[Java, Scala, C++]|\n",
      "|James  |[Spark, Java]     |\n",
      "|Michael|[Spark, Java, C++]|\n",
      "|Michael|[Spark, Java]     |\n",
      "|Robert |[CSharp, VB]      |\n",
      "|Robert |[Spark, Python]   |\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.select(df.name,explode(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to flatten the arrays, use flatten function which converts array of array columns to a single array on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------+\n",
      "|name   |flatten(subjects)              |\n",
      "+-------+-------------------------------+\n",
      "|James  |[Java, Scala, C++, Spark, Java]|\n",
      "|Michael|[Spark, Java, C++, Spark, Java]|\n",
      "|Robert |[CSharp, VB, Spark, Python]    |\n",
      "+-------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import flatten\n",
    "\n",
    "df.select(df.name,flatten(df.subjects)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a76f73",
   "metadata": {},
   "source": [
    "### Explode vs Explode_outer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf5982",
   "metadata": {},
   "source": [
    "`explode` creates a row for each element in the array or map column by ignoring null or empty values in array whereas `explode_outer` returns all values in array or map including null or empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- names: string (nullable = true)\n",
      " |-- likes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+-----+-----+------------------+\n",
      "|index|names|likes             |\n",
      "+-----+-----+------------------+\n",
      "|1    |Luke |[baseball, Soccer]|\n",
      "|2    |Lucy |NULL              |\n",
      "+-----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "  (1, 'Luke',[\"baseball\",\"Soccer\"]),\n",
    "  (2, \"Lucy\",None)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['index','names','likes'])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "698e1b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|index|names|   likes|\n",
      "+-----+-----+--------+\n",
      "|    1| Luke|baseball|\n",
      "|    1| Luke|  Soccer|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df1 = df.select('index','names',explode('likes').alias('likes'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c06056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|index|names|   likes|\n",
      "+-----+-----+--------+\n",
      "|    1| Luke|baseball|\n",
      "|    1| Luke|  Soccer|\n",
      "|    2| Lucy|    NULL|\n",
      "+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "\n",
    "df1 = df.select('index','names',explode_outer('likes').alias('likes'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c06328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('likes').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b758c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.select('names').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
