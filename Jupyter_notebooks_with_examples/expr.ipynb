{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd073a81",
   "metadata": {},
   "source": [
    "## expr()\n",
    "PySpark `expr()` is a SQL function to execute SQL-like expressions and to use an existing DataFrame column value as an expression argument to Pyspark built-in functions. Most of the commonly used SQL functions are either part of the PySpark Column class or built-in `pyspark.sql.functions` API, besides these PySpark also supports many other SQL functions, so in order to use these, you have to use `expr()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2adfdefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 20:40:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"expr\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad333bb",
   "metadata": {},
   "source": [
    "### Concatenate Columns using || (similar to SQL)\n",
    "Use || to concatenate values from two string columns, you can use expr() expression to do exactly same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62120c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James,Bond|\n",
      "|Scott|Varsa|Scott,Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.withColumn(\"Name\",expr(\" col1 ||','|| col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3c50b",
   "metadata": {},
   "source": [
    "### Using SQL CASE WHEN with expr()\n",
    "Using `CASE WHEN` expression on `withColumn()` by using `expr()`, this example updates an existing column gender with the derived values, M for male, F for Female, and unknown for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "722ef7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|gender|\n",
      "+-------+------+\n",
      "|  James|     M|\n",
      "|Michael|     F|\n",
      "|    Jen|      |\n",
      "+-------+------+\n",
      "\n",
      "+-------+-------+\n",
      "|   name| gender|\n",
      "+-------+-------+\n",
      "|  James|   Male|\n",
      "|Michael| Female|\n",
      "|    Jen|unknown|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\"),(\"Michael\",\"F\"),(\"Jen\",\"\")]\n",
    "columns = [\"name\",\"gender\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "#Using CASE WHEN similar to SQL.\n",
    "df2 = df.withColumn(\"gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' WHEN gender = 'F' THEN 'Female' ELSE 'unknown' END\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34743984",
   "metadata": {},
   "source": [
    "### Using an Existing Column Value for Expression\n",
    "adds a number of months from an existing column instead of a Python constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08309e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|increment|\n",
      "+----------+---------+\n",
      "|2019-01-23|        1|\n",
      "|2019-06-24|        2|\n",
      "|2019-09-20|        3|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df=spark.createDataFrame(data).toDF(\"date\",\"increment\") \n",
    "df.show()\n",
    "\n",
    "#Add Month value from another column\n",
    "df.select(df.date,df.increment, expr(\"add_months(date,increment)\").alias(\"inc_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9fc11",
   "metadata": {},
   "source": [
    "use SQL like syntax to provide the alias name to the column expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ddbb83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  inc_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.date,df.increment, expr(\"\"\"add_months(date,increment) as inc_date\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724df90b",
   "metadata": {},
   "source": [
    "### Cast Function with expr()\n",
    "converts long data type to String type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf8736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- increment: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- str_increment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.select(\"date\",expr(\"cast(increment as string) as str_increment\")).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a393f8",
   "metadata": {},
   "source": [
    "### Arithmetic operations\n",
    "`expr()` is also used to provide arithmetic operations, below examples add value 5 to increment and creates a new column new_increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f5f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|increment|\n",
      "+----------+---------+\n",
      "|2019-01-23|        1|\n",
      "|2019-06-24|        2|\n",
      "|2019-09-20|        3|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            6|\n",
      "|2019-06-24|        2|            7|\n",
      "|2019-09-20|        3|            8|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.select(df.date,df.increment, expr(\"increment + 5 as new_increment\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282358b",
   "metadata": {},
   "source": [
    "### Using Filter with expr()\n",
    "Filter the DataFrame rows can done using expr() expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "359861c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 100|   2|\n",
      "| 200|3000|\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 500| 500|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(100,2),(200,3000),(500,500)] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.show()\n",
    "df.filter(expr(\"col1 == col2\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
