{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When\n",
    "PySpark supports a way to check multiple conditions in sequence and returns a value when the first condition met by using SQL like case when and `when()`. `otherwise()` expressions, these works similar to `Switch` and `if then else` statements.\n",
    "#### PySpark When Otherwise \n",
    "`when()` is a SQL function that returns a Column type and `otherwise()` is a function of Column, if `otherwise()` is not used, it returns a None/NULL value.\n",
    "#### PySpark SQL Case When\n",
    "This is similar to SQL expression, Usage: `CASE WHEN cond1 THEN result WHEN cond2 THEN result... ELSE result` END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/08 16:29:26 WARN Utils: Your hostname, javier-ubuntu, resolves to a loopback address: 127.0.1.1; using 10.0.0.205 instead (on interface wlx0013eff3e14d)\n",
      "25/08/08 16:29:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/08 16:29:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/08 16:29:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  NULL|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  NULL|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('when').getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using when() otherwise() on PySpark DataFrame.\n",
    "PySpark `when()` is SQL function, in order to use this first you should import and this returns a Column type, `otherwise()` is a function of Column, when `otherwise()` not used and none of the conditions met it assigns None (Null) value. Usage would be like `when(condition).otherwise(default)`.\n",
    "\n",
    "`when()` function take 2 parameters, first param takes a condition and second takes a literal value or Column, if condition evaluates to true then it returns a value from second param.\n",
    "\n",
    "The code below replaces the value of gender with a new derived value, when conditions are not matched, we are assigning \"\" a for null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "                                 .when(df.gender == \"F\",\"Female\")\n",
    "                                 .when(df.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df.gender))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df2 = df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n",
    "                        .when(df.gender == \"F\",\"Female\")\n",
    "                        .when(df.gender.isNull() ,\"\")\n",
    "                        .otherwise(df.gender).alias(\"new_gender\"))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### PySpark SQL Case When on DataFrame\n",
    "  Case When statement is used to execute a sequence of conditions and returns a value when the first condition is met, similar to `SWITH` and `IF THEN ELSE` statements. \n",
    "  \n",
    "  Similarly, PySpark SQL Case When statement can be used on DataFrame, below are some of the examples of using with `withColumn()`, `select()`, `selectExpr()` utilizing `expr()` function.\n",
    "\n",
    "#### Syntax of SQL CASE WHEN ELSE END\n",
    "\n",
    "CASE\n",
    "    WHEN condition1 THEN result_value1\n",
    "    WHEN condition2 THEN result_value2\n",
    "    -----\n",
    "    -----\n",
    "    ELSE result\n",
    "END;\n",
    "\n",
    "* CASE is the start of the expression\n",
    "* Clause WHEN takes a condition, if condition true it returns a value from THEN\n",
    "* If the condition is false it goes to the next condition and so on.\n",
    "* If none of the condition matches, it returns a value from the ELSE clause.\n",
    "* END is to end the expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|name   |gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|James  |M     |60000 |Male      |\n",
      "|Michael|M     |70000 |Male      |\n",
      "|Robert |NULL  |400000|          |\n",
      "|Maria  |F     |500000|Female    |\n",
      "|Jen    |      |NULL  |          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "#Using Case When on withColumn()\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "                                        \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "                                        \"ELSE gender END\"))\n",
    "               \n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "                                \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "                                \"ELSE gender END\").alias(\"new_gender\"))\n",
    "\n",
    "df4.show()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Case When on SQL Expression\n",
    "You can also use Case When with SQL statement after creating a temporary view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cse When on select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  NULL|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  NULL|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "\n",
    "spark.sql(\"select name, gender, salary, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "                                        \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "                                        \"ELSE gender END as new_gender from EMP\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------------+\n",
      "|name   |gender|salary|new_gender  |\n",
      "+-------+------+------+------------+\n",
      "|James  |M     |60000 |Straight    |\n",
      "|Michael|M     |70000 |Straight    |\n",
      "|Robert |NULL  |400000|non-declared|\n",
      "|Maria  |F     |500000|Straight    |\n",
      "|Jen    |      |NULL  |non-declared|\n",
      "+-------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "               \n",
    "df5 = df.withColumn(\"new_gender\", when((col(\"gender\") == 'M') | (col(\"gender\") == 'F'), \"Straight\")\n",
    "                                .when(col(\"gender\") == 'null', \"unknown\")\n",
    "                                .otherwise(\"non-declared\"))\n",
    "               \n",
    "\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
