{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6652a40",
   "metadata": {},
   "source": [
    "# Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "615798d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 15:52:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f656861",
   "metadata": {},
   "source": [
    "# Create an Empty DataFrame & RDD\n",
    "\n",
    "Create an empty PySpark DataFrame/RDD manually with or without schema (column names).\n",
    "While working with files, sometimes we may not receive a file for processing, however, we still need to create a DataFrame manually with the same schema we expect. If we don’t create with the same schema, our operations/transformations (like union’s) on DataFrame fail as we refer to the columns that may not present.\n",
    "\n",
    "To handle situations similar to these, we always need to create a DataFrame with the same schema, which means the same column names and datatypes regardless of the file exists or empty file processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb499923",
   "metadata": {},
   "source": [
    "### Create Empty RDD in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9b86279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmptyRDD[216] at emptyRDD at NativeMethodAccessorImpl.java:0\n",
      "ParallelCollectionRDD[217] at readRDDFromFile at PythonRDD.scala:297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 15:52:31 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#Creates Empty RDD\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "print(emptyRDD)\n",
    "\n",
    "#Diplays\n",
    "#EmptyRDD[188] at emptyRDD\n",
    "\n",
    "# Alternatively you can also get empty RDD by using spark.sparkContext.parallelize([]).\n",
    "\n",
    "#Creates Empty RDD using parallelize\n",
    "rdd2= spark.sparkContext.parallelize([])\n",
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43958d4",
   "metadata": {},
   "source": [
    "### Create Empty DataFrame with Schema (StructType)\n",
    " Create a schema using StructType and StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac4878c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField('firstname', StringType(), True),\n",
    "  StructField('middlename', StringType(), True),\n",
    "  StructField('lastname', StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63663c",
   "metadata": {},
   "source": [
    "Now use the empty RDD created above and pass it to createDataFrame() of SparkSession along with the schema for column names & data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f54c2bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf0860",
   "metadata": {},
   "source": [
    "### Convert Empty RDD to DataFrame\n",
    "can also create empty DataFrame by converting empty RDD to DataFrame using toDF()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79163f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert empty RDD to Dataframe\n",
    "df1 = emptyRDD.toDF(schema)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "015f99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DataFrame directly.\n",
    "df2 = spark.createDataFrame([], schema)\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8425ae6",
   "metadata": {},
   "source": [
    "###  Create Empty DataFrame without Schema (no columns)\n",
    "To create empty DataFrame with out schema (no columns) just create a empty schema and use it while creating PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51ea1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create empty DatFrame with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b152a",
   "metadata": {},
   "source": [
    "### Create DataFrame from RDD\n",
    "create PySpark DataFrame from an existing RDD. First, let’s create a Spark RDD from a collection List by calling parallelize() function from SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d99db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd2 = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682ab620",
   "metadata": {},
   "source": [
    "#### Using toDF() function\n",
    "PySpark RDD’s toDF() method is used to create a DataFrame from existing RDD. Since RDD doesn’t have columns, the DataFrame is created with default column names “_1” and “_2” as we have two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1017da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n",
      "+------+------+\n",
      "|_1    |_2    |\n",
      "+------+------+\n",
      "|Java  |20000 |\n",
      "|Python|100000|\n",
      "|Scala |3000  |\n",
      "+------+------+\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()\n",
    "dfFromRDD1.show(truncate=False)\n",
    "\n",
    "dfFromRDD2 = rdd2.toDF()\n",
    "dfFromRDD2.printSchema()\n",
    "dfFromRDD2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8bd00f",
   "metadata": {},
   "source": [
    "If we want to provide column names to the DataFrame use toDF() method with column names as arguments as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d4a93c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|Java    |20000      |\n",
      "|Python  |100000     |\n",
      "|Scala   |3000       |\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()\n",
    "dfFromRDD1.show(truncate=False)\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd2.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265bf3eb",
   "metadata": {},
   "source": [
    "#### Using createDataFrame() from SparkSession\n",
    "Using createDataFrame() from SparkSession is another way to create manually and it takes rdd object as an argument, and chain with toDF() to specify name to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eaf50261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|Java    |20000      |\n",
      "|Python  |100000     |\n",
      "|Scala   |3000       |\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()\n",
    "dfFromRDD2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04eae6",
   "metadata": {},
   "source": [
    "### Create DataFrame from List Collection\n",
    "we use the list data object instead of “rdd” object to create DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c98d",
   "metadata": {},
   "source": [
    "#### Using createDataFrame() from SparkSession\n",
    "Calling createDataFrame() from SparkSession is another way to create PySpark DataFrame manually, it takes a list object as an argument. and chain with toDF() to specify names to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cc06e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|Java    |20000      |\n",
      "|Python  |100000     |\n",
      "|Scala   |3000       |\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)\n",
    "dfFromData2.printSchema()\n",
    "dfFromData2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df3f92b",
   "metadata": {},
   "source": [
    "#### Using createDataFrame() with the Row type\n",
    "createDataFrame() has another signature in PySpark which takes the collection of Row type and schema for column names as arguments. To use this first we need to convert our “data” object from the list to list of Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d88be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|Java    |20000      |\n",
      "|Python  |100000     |\n",
      "|Scala   |3000       |\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "rowData = map(lambda x: Row(*x), data) \n",
    "dfFromData3 = spark.createDataFrame(rowData,columns)\n",
    "dfFromData3.printSchema()\n",
    "dfFromData3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e66c05",
   "metadata": {},
   "source": [
    "#### Create Empty DataFrame with Schema\n",
    "If you wanted to specify the column names along with their data types, you should create the StructType schema first and then assign this while creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1f82238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "data2 = [(\"James\", \"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "         (\"Michael\", \"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "         (\"Robert\", \"\", \"Williams\", \"42114\", \"M\", 4000),\n",
    "         (\"Maria\", \"Anne\", \"Jones\", \"39192\", \"F\", 4000),\n",
    "         (\"Jen\", \"Mary\", \"Brown\", \"\", \"F\", -1)\n",
    "         ]\n",
    "\n",
    "# If you wanted to specify the column names along with their data types, you should\n",
    "# create the StructType schema first and then assign this while creating a DataFrame.\n",
    "schema = StructType([ \\\n",
    "    StructField(\"firstname\", StringType(), True), \\\n",
    "    StructField(\"middlename\", StringType(), True), \\\n",
    "    StructField(\"lastname\", StringType(), True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "    ])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data=data2, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68775abb",
   "metadata": {},
   "source": [
    "### Create DataFrame from Data sources\n",
    "You can manually create a PySpark DataFrame using toDF() and createDataFrame() methods\n",
    "Can also be created  from data sources like TXT, CSV, JSON, ORV, Avro, Parquet, XML formats by reading from HDFS, S3, DBFS, Azure Blob file systems, and by reading data from RDBMS Databases and NoSQL databases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7b5c2",
   "metadata": {},
   "source": [
    "#### Creating DataFrame from CSV\n",
    "Use csv() method of the DataFrameReader object to create a DataFrame from CSV file. you can also provide options like what delimiter to use, whether you have quoted data, date formats, infer schema, and many more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa102344",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+--------------------+---------------+-------------------+----------+-----+\n",
      "|RecordNumber|Zipcode|ZipCodeType|               City|State|  LocationType|  Lat|  Long|Xaxis|Yaxis|Zaxis|WorldRegion|Country|        LocationText|            Location|       Decommisioned|TaxReturnsFiled|EstimatedPopulation|TotalWages|Notes|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+--------------------+---------------+-------------------+----------+-----+\n",
      "|           1|    704|   STANDARD|        PARC PARQUE|   PR|NOT ACCEPTABLE|17.96|-66.22| null|-0.87|  0.3|         NA|     US|         Parc Parque|                  PR|NA-US-PR-PARC PARQUE|          false|               null|      null| null|\n",
      "|           2|    704|   STANDARD|PASEO COSTA DEL SUR|   PR|NOT ACCEPTABLE|17.96|-66.22| null|-0.87|  0.3|         NA|     US|Paseo Costa Del S...|NA-US-PR-PASEO CO...|               false|           null|               null|      null| null|\n",
      "|          10|    709|   STANDARD|       BDA SAN LUIS|   PR|NOT ACCEPTABLE|18.14|-66.26| null|-0.86| 0.31|         NA|     US|        Bda San Luis|                  PR|NA-US-PR-BDA SAN ...|          false|               null|      null| null|\n",
      "|       61391|  76166|     UNIQUE|  CINGULAR WIRELESS|   TX|NOT ACCEPTABLE|32.72|-97.31| null|-0.83| 0.54|         NA|     US|Cingular Wireless...|NA-US-TX-CINGULAR...|               false|           null|               null|      null| null|\n",
      "+------------+-------+-----------+-------------------+-----+--------------+-----+------+-----+-----+-----+-----------+-------+--------------------+--------------------+--------------------+---------------+-------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.csv(\"./resources/zipcodes.csv\", header=True)\n",
    "df2.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91812d2",
   "metadata": {},
   "source": [
    "### Convert PySpark DataFrame to Pandas\n",
    "Main differences between the Pandas & PySpark, operations on Pyspark run faster than Pandas due to its distributed nature and parallel execution on multiple cores and machines.\n",
    "\n",
    "In other words, pandas run operations on a single node whereas PySpark runs on multiple machines. If you are working on a Machine Learning application where you are dealing with larger datasets, PySpark processes operations many times faster than pandas.\n",
    "#### Prepare PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52d52489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f566209",
   "metadata": {},
   "source": [
    "#### Convert PySpark Dataframe to Pandas DataFrame\n",
    "PySpark DataFrame provides a method toPandas() to convert it Python Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "516a551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd44407",
   "metadata": {},
   "source": [
    "### Convert Spark Nested Struct DataFrame to Pandas\n",
    "Most of the time data in PySpark DataFrame will be in a structured format meaning one column contains other columns so let’s see how it convert to Pandas. Here is an example with nested struct where we have firstname, middlename and lastname are part of the name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4fda3ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+------+------+\n",
      "|name                |dob  |gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|{James, , Smith}    |36636|M     |3000  |\n",
      "|{Michael, Rose, }   |40288|M     |4000  |\n",
      "|{Robert, , Williams}|42114|M     |4000  |\n",
      "|{Maria, Anne, Jones}|39192|F     |4000  |\n",
      "|{Jen, Mary, Brown}  |     |F     |-1    |\n",
      "+--------------------+-----+------+------+\n",
      "\n",
      "                   name    dob gender salary\n",
      "0      (James, , Smith)  36636      M   3000\n",
      "1     (Michael, Rose, )  40288      M   4000\n",
      "2  (Robert, , Williams)  42114      M   4000\n",
      "3  (Maria, Anne, Jones)  39192      F   4000\n",
      "4    (Jen, Mary, Brown)             F     -1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "dataStruct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
    "]\n",
    "\n",
    "schemaStruct = StructType([\n",
    "      StructField('name', StructType([\n",
    "            StructField('firstname', StringType(), True),\n",
    "            StructField('middlename', StringType(), True),\n",
    "            StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "      StructField('dob', StringType(), True),\n",
    "            StructField('gender', StringType(), True),\n",
    "            StructField('salary', StringType(), True)\n",
    "         ])\n",
    "df = spark.createDataFrame(data=dataStruct, schema = schemaStruct)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "pandasDF2 = df.toPandas()\n",
    "print(pandasDF2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
