{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6410d240",
   "metadata": {},
   "source": [
    "## Groupby\n",
    "Similar to SQL GROUP BY clause, PySpark `groupBy()` function is used to collect the identical data into groups on DataFrame and perform aggregate functions on the grouped data\n",
    "\n",
    "When we perform `groupBy()` on PySpark Dataframe, it returns GroupedData object which contains below aggregate functions.\n",
    "\n",
    "* `count()` - Returns the count of rows for each group.\n",
    "\n",
    "* `mean()` - Returns the mean of values for each group.\n",
    "\n",
    "* `max()` - Returns the maximum of values for each group.\n",
    "\n",
    "* `min()` - Returns the minimum of values for each group.\n",
    "\n",
    "* `sum()` - Returns the total for values for each group.\n",
    "\n",
    "* `avg()` - Returns the average for values for each group.\n",
    "\n",
    "* `agg()` - Using agg() function, we can calculate more than one aggregate at a time.\n",
    "\n",
    "* `pivot()` - This function is used to Pivot the DataFrame which I will not be covered in this article as I already have a dedicated article for Pivot & Unpivot DataFrame.\n",
    "\n",
    "Create DataFrame from a sequence of the data to work with. This DataFrame contains columns `employee_name`, `department`, `state`, `salary`, `age` and `bonus` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6e577c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/10 17:12:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"groupby\").getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "              (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "              (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "              (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "              (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "              (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "              (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "              (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "              (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae18726",
   "metadata": {},
   "source": [
    "### groupBy and aggregate on DataFrame columns\n",
    "Let’s do the `groupBy()` on `department` column of DataFrame and then find the sum of `salary` for each department using `sum()` aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b2b851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4220d6",
   "metadata": {},
   "source": [
    "Similarly, we can calculate the number of employee in each department using `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d28a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|department|count|\n",
      "+----------+-----+\n",
      "|     Sales|    3|\n",
      "|   Finance|    4|\n",
      "| Marketing|    2|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fc3509",
   "metadata": {},
   "source": [
    "Calculate the minimum salary of each department using `min()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aacb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|min(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      81000|\n",
      "|   Finance|      79000|\n",
      "| Marketing|      80000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1b4d4d",
   "metadata": {},
   "source": [
    "Calculate the maximin salary of each department using `max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23e4194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|max(salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      90000|\n",
      "|   Finance|      99000|\n",
      "| Marketing|      91000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a35f9a",
   "metadata": {},
   "source": [
    "Calculate the average salary of each department using `avg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba92d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").avg( \"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a896d9",
   "metadata": {},
   "source": [
    "Calculate the mean salary of each department using `mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0bc7061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|      avg(salary)|\n",
      "+----------+-----------------+\n",
      "|     Sales|85666.66666666667|\n",
      "|   Finance|          87750.0|\n",
      "| Marketing|          85500.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").mean( \"salary\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdde50f",
   "metadata": {},
   "source": [
    "### groupBy and aggregate on multiple columns\n",
    "Similarly, we can also run groupBy and aggregate on two or more DataFrame columns, below example does group by on `department`,`stat`e and does `sum()` on `salary` and `bonus` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c1c5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|   Finance|   NY|     162000|     34000|\n",
      "| Marketing|   NY|      91000|     21000|\n",
      "|     Sales|   CA|      81000|     23000|\n",
      "| Marketing|   CA|      80000|     18000|\n",
      "|   Finance|   CA|     189000|     47000|\n",
      "|     Sales|   NY|     176000|     30000|\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GroupBy on multiple columns\n",
    "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6ecc1",
   "metadata": {},
   "source": [
    "### Running more aggregates at a time\n",
    "Using `agg()` aggregate function we can calculate many aggregations at a time on a single statement using PySpark SQL aggregate functions `sum()`, `avg()`, `min()`, `max()`, `mean()` etc. In order to use these, we should import \"from pyspark.sql.functions import sum,avg,max,min,mean,count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9581d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "|Marketing |171000    |85500.0          |39000    |21000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c13a0",
   "metadata": {},
   "source": [
    "### Using filter on aggregate data\n",
    "Similar to SQL “HAVING” clause, On PySpark DataFrame we can use either `where()` or `filter()` function to filter the rows of aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "060d4465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+---------+---------+\n",
      "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
      "|Finance   |351000    |87750.0          |81000    |24000    |\n",
      "+----------+----------+-----------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\")).where(col(\"sum_bonus\") >= 50000).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0c6ad",
   "metadata": {},
   "source": [
    "## -------------- Second example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ea95cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Job: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- seniority: long (nullable = true)\n",
      "\n",
      "+--------+--------------+-------+------+---------+\n",
      "|Name    |Job           |Country|salary|seniority|\n",
      "+--------+--------------+-------+------+---------+\n",
      "|Carol   |Data Scientist|USA    |70000 |5        |\n",
      "|Peter   |Data Scientist|USA    |90000 |7        |\n",
      "|Clark   |Data Scientist|UK     |111000|10       |\n",
      "|Jean    |Data Scientist|UK     |220000|30       |\n",
      "|Bruce   |Data Engineer |UK     |80000 |4        |\n",
      "|Thanos  |Data Engineer |USA    |115000|13       |\n",
      "|Scott   |Data Engineer |UK     |180000|15       |\n",
      "|T'challa|CEO           |USA    |300000|20       |\n",
      "|Xavier  |Marketing     |USA    |100000|11       |\n",
      "|Wade    |Marketing     |UK     |60000 |2        |\n",
      "+--------+--------------+-------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/10 17:12:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "  \n",
    "datavengers = [\n",
    "    (\"Carol\",\"Data Scientist\",\"USA\",70000,5),\n",
    "    (\"Peter\",\"Data Scientist\",\"USA\",90000,7),\n",
    "    (\"Clark\",\"Data Scientist\",\"UK\",111000,10),\n",
    "    (\"Jean\",\"Data Scientist\",\"UK\",220000,30),\n",
    "    (\"Bruce\",\"Data Engineer\",\"UK\",80000,4),\n",
    "    (\"Thanos\",\"Data Engineer\",\"USA\",115000,13),\n",
    "    (\"Scott\",\"Data Engineer\",\"UK\",180000,15),\n",
    "    (\"T'challa\",\"CEO\",\"USA\",300000,20),\n",
    "    (\"Xavier\",\"Marketing\",\"USA\",100000,11),\n",
    "    (\"Wade\",\"Marketing\",\"UK\",60000,2)\n",
    "]\n",
    " \n",
    "schema = [\"Name\",\"Job\",\"Country\",\"salary\",\"seniority\"]\n",
    "df = spark.createDataFrame(data=datavengers, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5b2cf",
   "metadata": {},
   "source": [
    "### groupBy using count() function\n",
    "To count the number of employees per job type, you can proceed like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc7dbf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Job           |count|\n",
      "+--------------+-----+\n",
      "|CEO           |1    |\n",
      "|Data Scientist|4    |\n",
      "|Marketing     |2    |\n",
      "|Data Engineer |3    |\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Job\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81a12d7",
   "metadata": {},
   "source": [
    "### groupBy using sum() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9ec7453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|Job           |sum(salary)|\n",
      "+--------------+-----------+\n",
      "|CEO           |300000     |\n",
      "|Data Scientist|491000     |\n",
      "|Marketing     |160000     |\n",
      "|Data Engineer |375000     |\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Job\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31a8ea",
   "metadata": {},
   "source": [
    "### groupBy using avg() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "730e7f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|Job           |avg(salary)|\n",
      "+--------------+-----------+\n",
      "|CEO           |300000.0   |\n",
      "|Data Scientist|122750.0   |\n",
      "|Marketing     |80000.0    |\n",
      "|Data Engineer |125000.0   |\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Job\").avg(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ed570",
   "metadata": {},
   "source": [
    "### groupBy and aggregation functions on DataFrame multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "530ca675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-----------+--------------+\n",
      "|Job           |Country|avg(salary)|avg(seniority)|\n",
      "+--------------+-------+-----------+--------------+\n",
      "|Marketing     |UK     |60000.0    |2.0           |\n",
      "|Data Engineer |UK     |130000.0   |9.5           |\n",
      "|Data Scientist|UK     |165500.0   |20.0          |\n",
      "|Marketing     |USA    |100000.0   |11.0          |\n",
      "|Data Scientist|USA    |80000.0    |6.0           |\n",
      "|CEO           |USA    |300000.0   |20.0          |\n",
      "|Data Engineer |USA    |115000.0   |13.0          |\n",
      "+--------------+-------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Job\",\"Country\").avg(\"salary\",\"seniority\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
