{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Window Functions\n",
    "\n",
    "Pyspark window functions are useful when you want to examine relationships within groups of data rather than between groups of data (as for groupBy)\n",
    "\n",
    "To use them you start by defining a window function then select a separate function or set of functions to operate within that window\n",
    "\n",
    "NB- this workbook is designed to work on Databricks Community Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-----------+-------+-------+----------+\n",
      "|partition|col_1|aggregation|ranking|lagging|cumulative|\n",
      "+---------+-----+-----------+-------+-------+----------+\n",
      "|        a|    1|          1|      4|      9|         1|\n",
      "|        a|    1|          2|      3|      8|         2|\n",
      "|        a|    1|          3|      2|      7|         4|\n",
      "|        a|    1|          4|      1|      6|         6|\n",
      "|        b|    2|          5|      1|      5|         1|\n",
      "|        b|    2|          6|      1|      4|         1|\n",
      "|        b|    2|          7|      3|      3|         1|\n",
      "|        c|    3|          8|      1|      2|        20|\n",
      "|        c|    3|          9|      5|      1|        30|\n",
      "+---------+-----+-----------+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session\n",
    "spark_session = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# lets define a demonstration DataFrame to work on\n",
    "df_data = {'partition': ['a','a', 'a', 'a', 'b', 'b', 'b', 'c', 'c',],\n",
    "           'col_1': [1,1,1,1,2,2,2,3,3,], \n",
    "           'aggregation': [1,2,3,4,5,6,7,8,9,],\n",
    "           'ranking': [4,3,2,1,1,1,3,1,5,],\n",
    "           'lagging': [9,8,7,6,5,4,3,2,1,],\n",
    "           'cumulative': [1,2,4,6,1,1,1,20,30,],\n",
    "          }\n",
    "df_pandas = pd.DataFrame.from_dict(df_data)\n",
    "# create spark dataframe\n",
    "df = spark_session.createDataFrame(df_pandas)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple aggregation functions\n",
    "\n",
    "we can use the standard group by aggregations with window functions. These functions use the simplest form of window which just defines grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+---------------+---------------+---------------+\n",
      "|partition|aggregation|aggregation_sum|aggregation_avg|aggregation_min|aggregation_max|\n",
      "+---------+-----------+---------------+---------------+---------------+---------------+\n",
      "|        a|          1|             10|            2.5|              1|              4|\n",
      "|        a|          2|             10|            2.5|              1|              4|\n",
      "|        a|          3|             10|            2.5|              1|              4|\n",
      "|        a|          4|             10|            2.5|              1|              4|\n",
      "|        b|          5|             18|            6.0|              5|              7|\n",
      "|        b|          6|             18|            6.0|              5|              7|\n",
      "|        b|          7|             18|            6.0|              5|              7|\n",
      "|        c|          8|             17|            8.5|              8|              9|\n",
      "|        c|          9|             17|            8.5|              8|              9|\n",
      "+---------+-----------+---------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregation functions use the simplest form of window which just defines grouping\n",
    "aggregation_window = Window.partitionBy('partition')\n",
    "\n",
    "# then we can use this window function for our aggregations\n",
    "df_aggregations = df.select('partition', 'aggregation')\\\n",
    "      .withColumn('aggregation_sum', fn.sum('aggregation').over(aggregation_window))\\\n",
    "      .withColumn('aggregation_avg', fn.avg('aggregation').over(aggregation_window))\\\n",
    "      .withColumn('aggregation_min', fn.min('aggregation').over(aggregation_window))\\\n",
    "      .withColumn('aggregation_max', fn.max('aggregation').over(aggregation_window))\n",
    "\n",
    "df_aggregations.show()\n",
    "# note that after this operation the row order of display within the dataframe may have changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row wise ordering and ranking functions\n",
    "\n",
    "We can also use window funtions to order and rank data. These functions add an element to the definition of the window which defines both grouping AND ordering  \n",
    "\n",
    "* note that `row_number()` does not take any arguments\n",
    "* `rank` will leave spaces in ranking to account for preceding rows receiving equal ranks\n",
    "* `dense rank` does not account for previous equal rankings\n",
    "* `percent rank` ranges between 0-1 not 0-100\n",
    "* `ntile` takes a parameter for now many 'buckets' to divide rows into when ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------------+------------+------------------+--------------------+------------------+\n",
      "|partition|ranking|ranking_row_number|ranking_rank|ranking_dense_rank|ranking_percent_rank|ranking_ntile_rank|\n",
      "+---------+-------+------------------+------------+------------------+--------------------+------------------+\n",
      "|        a|      1|                 1|           1|                 1|                 0.0|                 1|\n",
      "|        a|      2|                 2|           2|                 2|  0.3333333333333333|                 1|\n",
      "|        a|      3|                 3|           3|                 3|  0.6666666666666666|                 2|\n",
      "|        a|      4|                 4|           4|                 4|                 1.0|                 2|\n",
      "|        b|      1|                 1|           1|                 1|                 0.0|                 1|\n",
      "|        b|      1|                 2|           1|                 1|                 0.0|                 1|\n",
      "|        b|      3|                 3|           3|                 2|                 1.0|                 2|\n",
      "|        c|      1|                 1|           1|                 1|                 0.0|                 1|\n",
      "|        c|      5|                 2|           2|                 2|                 1.0|                 2|\n",
      "+---------+-------+------------------+------------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets define a ranking window\n",
    "ranking_window = Window.partitionBy('partition').orderBy('ranking')\n",
    "\n",
    "df_ranks = df.select('partition', 'ranking')\\\n",
    "                .withColumn('ranking_row_number', fn.row_number().over(ranking_window))\\\n",
    "                .withColumn('ranking_rank', fn.rank().over(ranking_window))\\\n",
    "                .withColumn('ranking_dense_rank', fn.dense_rank().over(ranking_window))\\\n",
    "                .withColumn('ranking_percent_rank', fn.percent_rank().over(ranking_window))\\\n",
    "                .withColumn('ranking_ntile_rank', fn.ntile(2).over(ranking_window))\n",
    "\n",
    "\n",
    "df_ranks.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating lagged columns\n",
    "\n",
    "If we want to conduct operations like calculating the difference between subsequent operations in a group, we can use window functions to create the lagged values we require to perform the calculation. Where there is no preceding lag value, a null entry will be inserted not a zero.\n",
    "\n",
    "The inverse of lag is lead. Effectively fn.lag(n) == fn.lead(-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-------------+--------------+--------------------+------------------+\n",
      "|partition|lagging|lagging_lag_1|lagging_lag_2|lagging_lead_1|lagging_lead_minus_1|difference_between|\n",
      "+---------+-------+-------------+-------------+--------------+--------------------+------------------+\n",
      "|        a|      6|         NULL|         NULL|             7|                NULL|              NULL|\n",
      "|        a|      7|            6|         NULL|             8|                   6|                 1|\n",
      "|        a|      8|            7|            6|             9|                   7|                 1|\n",
      "|        a|      9|            8|            7|          NULL|                   8|                 1|\n",
      "|        b|      3|         NULL|         NULL|             4|                NULL|              NULL|\n",
      "|        b|      4|            3|         NULL|             5|                   3|                 1|\n",
      "|        b|      5|            4|            3|          NULL|                   4|                 1|\n",
      "|        c|      1|         NULL|         NULL|             2|                NULL|              NULL|\n",
      "|        c|      2|            1|         NULL|          NULL|                   1|                 1|\n",
      "+---------+-------+-------------+-------------+--------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lag_window = Window.partitionBy('partition').orderBy('lagging')\n",
    "\n",
    "df_lagged = df.select('partition', 'lagging'\n",
    "                    ).withColumn('lagging_lag_1', fn.lag('lagging', 1).over(lag_window)\n",
    "                      # note that lag requires both column and lag amount to be specified\n",
    "                      # It is possible to lag a column which was not the orderBy column\n",
    "                    ).withColumn('lagging_lag_2', fn.lag('lagging', 2).over(lag_window)\n",
    "                    ).withColumn('lagging_lead_1', fn.lead('lagging', 1).over(lag_window)\n",
    "                    ).withColumn('lagging_lead_minus_1', fn.lead('lagging', -1).over(lag_window)\n",
    "                      # note how 'lagging_lag_1' == 'lagging_lead_minus_1'\n",
    "                    ).withColumn('difference_between', fn.col('lagging') - fn.lag('lagging', 1).over(lag_window)\n",
    "                      # we can also perform calculations between lagged and unlagged columns of course\n",
    "                    )\n",
    "\n",
    "df_lagged.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Calculations (Running totals and averages)\n",
    "\n",
    "There are often good reasons to want to create a running total or running average column. In some cases we might want running totals for subsets of data. Window functions can be useful for that sort of thing. \n",
    "\n",
    "In order to calculate such things we need to add yet another element to the window. Now we account for partition, order and which rows should be covered by the function. This can be done in two ways we can use **rangeBetween** to define how similar values in the window must be to be considered, or we can use **rowsBetween** to define how many rows should be considered. The current row is considered row zero, the following rows are numbered positively and the preceding rows negatively. For cumulative calculations you can define \"all previous rows\" with **Window.unboundedPreceding** and \"all following rows\" with **Window.unboundedFolowing**\n",
    "\n",
    "Note that the window may vary in size as it progresses over the rows since at the start and end part of the window may \"extend past\" the existing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n",
      "|partition|cumulative|    cumulative_avg|\n",
      "+---------+----------+------------------+\n",
      "|        a|         1|               1.5|\n",
      "|        a|         2|2.3333333333333335|\n",
      "|        a|         4|               4.0|\n",
      "|        a|         6|               5.0|\n",
      "|        b|         1|               1.0|\n",
      "|        b|         1|               1.0|\n",
      "|        b|         1|               1.0|\n",
      "|        c|        20|              25.0|\n",
      "|        c|        30|              25.0|\n",
      "+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#suppose we want to average over the previous, current and next values\n",
    "# running calculations need a more complicated window as shown here\n",
    "# for a rolling average lets use rowsBetween\n",
    "\n",
    "cumulative_window_1 = Window.partitionBy('partition').orderBy('cumulative').rowsBetween(  -1,1)\n",
    "\n",
    "\n",
    "df_cumulative_1 = df.select('partition', 'cumulative').withColumn('cumulative_avg', fn.avg('cumulative').over(cumulative_window_1))\n",
    "\n",
    "df_cumulative_1.show()\n",
    "# note how the averages don't use 3 rows at the ends of the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------+\n",
      "|partition|cumulative|cumulative_sum|\n",
      "+---------+----------+--------------+\n",
      "|        a|         1|             1|\n",
      "|        a|         2|             3|\n",
      "|        a|         4|             7|\n",
      "|        a|         6|            13|\n",
      "|        b|         1|             3|\n",
      "|        b|         1|             3|\n",
      "|        b|         1|             3|\n",
      "|        c|        20|            20|\n",
      "|        c|        30|            50|\n",
      "+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# running totals also require a more complicated window as here. \n",
    "cumulative_window_2 = Window.partitionBy('partition').orderBy('cumulative'\n",
    "                        # in this case we will use rangeBetween for the sum\n",
    "                        ).rangeBetween(Window.unboundedPreceding, 0)\n",
    "# In this case we need to use Window.unboundedPreceding to catch all earlier rows\n",
    "  \n",
    "df_cumulative_2 = df.select('partition', 'cumulative').withColumn('cumulative_sum', fn.sum('cumulative').over(cumulative_window_2))\n",
    "\n",
    "df_cumulative_2.show()\n",
    "# note the summing behaviour where multiple identical values are present in the orderBy column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Windows and Calling Different Columns\n",
    "It is also possible to combine windows and also to call windows on columns other than the ordering column. These more advanced uses can require careful thought to ensure you achieve the intended results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|partition|aggregation_sum|aggregation_avg|aggregation_min|aggregation_max|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|        a|             10|            2.5|              1|              4|\n",
      "|        b|             18|            6.0|              5|              7|\n",
      "|        c|             17|            8.5|              8|              9|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|partition|aggregation_sum|aggregation_avg|aggregation_min|aggregation_max|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "|        a|             10|            2.5|              1|              4|\n",
      "|        b|             18|            6.0|              5|              7|\n",
      "|        c|             17|            8.5|              8|              9|\n",
      "+---------+---------------+---------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can make a window function equivalent to a standard groupBy:\n",
    "\n",
    "# first define two windows\n",
    "aggregation_window = Window.partitionBy('partition')\n",
    "grouping_window = Window.partitionBy('partition').orderBy('aggregation')\n",
    "\n",
    "# then we can use this window function for our aggregations\n",
    "\n",
    "  # note that we calculate row number over the grouping_window\n",
    "  # but we calculate other columns over the aggregation_window\n",
    "  \n",
    "df_aggregations = df.select('partition', 'aggregation')\\\n",
    "                      .withColumn('group_rank', fn.row_number().over(grouping_window))\\\n",
    "                      .withColumn('aggregation_sum', fn.sum('aggregation').over(aggregation_window))\\\n",
    "                      .withColumn('aggregation_avg', fn.avg('aggregation').over(aggregation_window))\\\n",
    "                      .withColumn('aggregation_min', fn.min('aggregation').over(aggregation_window))\\\n",
    "                      .withColumn('aggregation_max', fn.max('aggregation').over(aggregation_window))\\\n",
    "                      .where( fn.col('group_rank') == 1)\\\n",
    "                      .select('partition', 'aggregation_sum','aggregation_avg','aggregation_min', 'aggregation_max')\n",
    "\n",
    "\n",
    "\n",
    "df_aggregations.show()\n",
    "\n",
    "# this is equivalent to the rather simpler expression below\n",
    "df_groupby = df.select('partition', 'aggregation').groupBy('partition').agg(\n",
    "                    fn.sum('aggregation').alias('aggregation_sum'),\n",
    "                    fn.avg('aggregation').alias('aggregation_avg'),\n",
    "                    fn.min('aggregation').alias('aggregation_min'),\n",
    "                    fn.max('aggregation').alias('aggregation_max'))\n",
    "\n",
    "df_groupby.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+----------+--------------------+----------------------+\n",
      "|partition|lagging|cumulative|lag_the_laggging_col|lag_the_cumulative_col|\n",
      "+---------+-------+----------+--------------------+----------------------+\n",
      "|        a|      6|         6|                NULL|                  NULL|\n",
      "|        a|      7|         4|                   6|                     6|\n",
      "|        a|      8|         2|                   7|                     4|\n",
      "|        a|      9|         1|                   8|                     2|\n",
      "|        b|      3|         1|                NULL|                  NULL|\n",
      "|        b|      4|         1|                   3|                     1|\n",
      "|        b|      5|         1|                   4|                     1|\n",
      "|        c|      1|        30|                NULL|                  NULL|\n",
      "|        c|      2|        20|                   1|                    30|\n",
      "+---------+-------+----------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in some cases we can create a window on one column but use the window on another column \n",
    "# note that only functions where the column is specified allow this\n",
    "lag_window = Window.partitionBy('partition').orderBy('lagging')\n",
    "\n",
    "df_cumulative_2 = df.select('partition', 'lagging', 'cumulative')\\\n",
    "    .withColumn('lag_the_laggging_col', fn.lag('lagging', 1).over(lag_window))\\\n",
    "    .withColumn('lag_the_cumulative_col', fn.lag('cumulative', 1).over(lag_window))  # It is possible to lag a column which was not the orderBy column\n",
    "\n",
    "df_cumulative_2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
