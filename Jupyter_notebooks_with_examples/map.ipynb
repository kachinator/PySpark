{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbcd2f70",
   "metadata": {},
   "source": [
    "## map() Transformation\n",
    "`map()` is an RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD\n",
    "\n",
    "RDD `map()` transformation is used to apply any complex operations like adding a column, updating a column, transforming the data e.t.c, the output of map transformations would always have the same number of records as input.\n",
    "\n",
    "* Note1: DataFrame doesn’t have `map()` transformation to use with DataFrame hence you need to DataFrame to RDD first.\n",
    "* Note2: If you have a heavy initialization use PySpark `mapPartitions()` transformation instead of `map()`, as with `mapPartitions()` heavy initialization executes only once for each partition instead of every record.\n",
    "\n",
    "Create an RDD from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "046ae3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"map\").getOrCreate()\n",
    "\n",
    "data = [\"Eggs\", \"are\", \"forgiving\", \"of\", \"forgetfulness.\", \"But\", \"butter,\", \"not\", \"so\", \"much\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd942e4",
   "metadata": {},
   "source": [
    "### map() with RDD\n",
    "We'll be adding a new element with value 1 for each element, the result of the RDD is PairRDDFunctions which contains key-value pairs, word of type String as Key and 1 of type Int as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6305392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Eggs', 'checked')\n",
      "('are', 'checked')\n",
      "('forgiving', 'checked')\n",
      "('of', 'checked')\n",
      "('forgetfulness.', 'checked')\n",
      "('But', 'checked')\n",
      "('butter,', 'checked')\n",
      "('not', 'checked')\n",
      "('so', 'checked')\n",
      "('much', 'checked')\n"
     ]
    }
   ],
   "source": [
    "rdd2=rdd.map(lambda x: (x,\"checked\"))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d108c",
   "metadata": {},
   "source": [
    "### map() with DataFrame\n",
    "PySpark DataFrame doesn’t have `map()` transformation to apply the lambda function, when you wanted to apply the custom transformation, you need to convert the DataFrame to RDD and apply the `map()` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c85a10b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|   130|\n",
      "|     Anna|   Trump|     F|   141|\n",
      "|   Robert|Williams|     M|   162|\n",
      "+---------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James','Smith','M',130),\n",
    "        ('Anna','Trump','F',141),\n",
    "        ('Robert','Williams','M',162)]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fde65",
   "metadata": {},
   "source": [
    "Refering columns by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95dcf5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:57 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|       260|\n",
      "|     Anna,Trump|     F|       282|\n",
      "|Robert,Williams|     M|       324|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd2 = df.rdd.map(lambda x: (x[0]+\",\"+x[1],x[2],x[3]*2))  \n",
    "df2 = rdd2.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1125b97",
   "metadata": {},
   "source": [
    "can also refer to the DataFrame column names while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b066b0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:58 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|       260|\n",
      "|     Anna,Trump|     F|       282|\n",
      "|Robert,Williams|     M|       324|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd3 = df.rdd.map(lambda x: (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)) \n",
    "df3 = rdd3.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbeb2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:58 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|       260|\n",
      "|     Anna,Trump|     F|       282|\n",
      "|Robert,Williams|     M|       324|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd4 = df.rdd.map(lambda x: (x.firstname + \",\" + x.lastname, x.gender, x.salary*2)) \n",
    "df4 = rdd4.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8c222",
   "metadata": {},
   "source": [
    "#### By Calling a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b91170fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:58 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     m|       260|\n",
      "|     Anna,Trump|     f|       282|\n",
      "|Robert,Williams|     m|       324|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func1(x):\n",
    "    firstName = x.firstname\n",
    "    lastName = x.lastname\n",
    "    name = firstName + \",\" + lastName\n",
    "    gender = x.gender.lower()\n",
    "    salary = x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "rdd5 = df.rdd.map(lambda x: func1(x))\n",
    "df5 = rdd5.toDF([\"name\",\"gender\",\"new_salary\"])\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18a7f5",
   "metadata": {},
   "source": [
    "### Convert DataFrame Columns to MapType (Dict)\n",
    "Convert selected or all DataFrame columns to MapType similar to Python Dictionary (Dict) object.\n",
    "\n",
    "function `create_map()` is used to convert selected DataFrame columns to `MapType`, `create_map()` takes a list of columns you wanted to convert as an argument and returns a MapType column.\n",
    "\n",
    "Let’s create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20394c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n",
      "+-----+---------+------+--------+\n",
      "|id   |dept     |salary|location|\n",
      "+-----+---------+------+--------+\n",
      "|36636|Finance  |13000 |USA     |\n",
      "|40288|Finance  |15000 |MEX     |\n",
      "|42114|Sales    |13900 |USA     |\n",
      "|39192|Marketing|12500 |CAN     |\n",
      "|34534|Sales    |16500 |USA     |\n",
      "+-----+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "data = [ (\"36636\",\"Finance\",13000,\"USA\"), \n",
    "         (\"40288\",\"Finance\",15000,\"MEX\"), \n",
    "         (\"42114\",\"Sales\",13900,\"USA\"), \n",
    "         (\"39192\",\"Marketing\",12500,\"CAN\"), \n",
    "         (\"34534\",\"Sales\",16500,\"USA\") ]\n",
    "\n",
    "schema = StructType([\n",
    "     StructField('id', StringType(), True),\n",
    "     StructField('dept', StringType(), True),\n",
    "     StructField('salary', IntegerType(), True),\n",
    "     StructField('location', StringType(), True)\n",
    "     ])\n",
    "\n",
    "df6 = spark.createDataFrame(data=data,schema=schema)\n",
    "df6.printSchema()\n",
    "df6.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e7f20",
   "metadata": {},
   "source": [
    "#### Convert DataFrame Columns to MapType\n",
    "using `create_map()` SQL function let’s convert PySpark DataFrame columns `salary` and `location` to `MapType`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "27dbd929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- propertiesMap: map (nullable = false)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+-----+------+------------------------------------+\n",
      "|id   |salary|propertiesMap                       |\n",
      "+-----+------+------------------------------------+\n",
      "|36636|13000 |{dept -> Finance, location -> USA}  |\n",
      "|40288|15000 |{dept -> Finance, location -> MEX}  |\n",
      "|42114|13900 |{dept -> Sales, location -> USA}    |\n",
      "|39192|12500 |{dept -> Marketing, location -> CAN}|\n",
      "|34534|16500 |{dept -> Sales, location -> USA}    |\n",
      "+-----+------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit,create_map\n",
    "\n",
    "df7 = df6.withColumn(\"propertiesMap\",create_map(lit(\"dept\"),col(\"dept\"),  \n",
    "                                                lit(\"location\"),col(\"location\"))).drop(\"dept\",\"location\")\n",
    "                                      \n",
    "                    \n",
    "df7.printSchema()\n",
    "df7.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a16e852",
   "metadata": {},
   "source": [
    "#### Using foreach() to Loop Through Rows in DataFrame\n",
    "Similar to `map()`, `foreach()` also applied to every row of DataFrame, the difference being `foreach()` is an action and it returns nothing. Below are some examples to iterate through DataFrame using for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0b34aceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "Data ==> James,Smith,M,260\n",
      "Data ==> Anna,Trump,F,282\n",
      "Data ==> Robert,Williams,M,324\n"
     ]
    }
   ],
   "source": [
    "df.foreach(lambda x: print(\"Data ==> \"+x[\"firstname\"]+\",\"+x[\"lastname\"]+\",\"+x[\"gender\"]+\",\"+str(x[\"salary\"]*2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83394768",
   "metadata": {},
   "source": [
    "* Data ==>Robert,Williams,M,124\n",
    "* Data ==>James,Smith,M,60\n",
    "* Data ==>Anna,Rose,F,82"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1be0e9",
   "metadata": {},
   "source": [
    "#### Using pandas() to Iterate\n",
    "If you have a small dataset, you can also Convert PySpark DataFrame to Pandas and use pandas to iterate through. Use `spark.sql.execution.arrow.enabled` config to enable Apache Arrow with Spark. Apache Spark uses Apache Arrow which is an in-memory columnar format to transfer the data between Python and JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1787351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James M\n",
      "Anna F\n",
      "Robert M\n"
     ]
    }
   ],
   "source": [
    "# Using pandas\n",
    "import pandas as pd\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "pandasDF = df.toPandas()\n",
    "\n",
    "for index, row in pandasDF.iterrows():\n",
    "    print(row['firstname'], row['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ddab0",
   "metadata": {},
   "source": [
    "#### Collect Data As List and Loop Through\n",
    "You can also Collect the PySpark DataFrame to Driver and iterate through Python, you can also use `toLocalIterator()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9982b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mJames,Smith\u001b[0m\n",
      "\u001b[31mAnna,Trump\u001b[0m\n",
      "\u001b[31mRobert,Williams\u001b[0m\n",
      "\u001b[34mJames,Smith\u001b[0m\n",
      "\u001b[34mAnna,Trump\u001b[0m\n",
      "\u001b[34mRobert,Williams\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/09 15:44:59 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "# Collect the data to Python List\n",
    "from termcolor import cprint\n",
    "\n",
    "dataCollect = df.collect()\n",
    "for row in dataCollect:\n",
    "    cprint(row['firstname'] + \",\" +row['lastname'], 'red')\n",
    "\n",
    "#Using toLocalIterator()\n",
    "dataCollect=df.rdd.toLocalIterator()\n",
    "for row in dataCollect:\n",
    "    cprint(row['firstname'] + \",\" +row['lastname'], 'blue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
