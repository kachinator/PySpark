{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a797bc3",
   "metadata": {},
   "source": [
    "## Row class usage for DataFrame and RDD\n",
    "`Row` class is available by importing `pyspark.sql.Row` which is represented as a record/row in DataFrame, one can create a `Row` object by using named arguments, or create a custom `Row` like class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca601c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/08 17:26:22 WARN Utils: Your hostname, javier-ubuntu, resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "25/08/08 17:26:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/08 17:26:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/08 17:26:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName('row').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf5f01",
   "metadata": {},
   "source": [
    "### Create a Row Object\n",
    "Row class extends the tuple hence it takes variable number of arguments, Row() is used to create the row object. Once the row object created, we can retrieve the data from Row using index similar to tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6707a5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,40\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "row = Row(\"James\",40)\n",
    "print(row[0] +\",\"+str(row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525406a6",
   "metadata": {},
   "source": [
    "write with named arguments. Benefits with the named argument is you can access with field name `row.name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c92e472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "row = Row(name=\"Alice\", age=11)\n",
    "print(row.name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656dc8e",
   "metadata": {},
   "source": [
    "### Create Custom Class from Row\n",
    "create a Row like class, for example “Person” and use it similar to Row object. This would be helpful when you wanted to create real time object and refer it’s properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc0ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,Alice\n"
     ]
    }
   ],
   "source": [
    "Person = Row(\"name\", \"age\")\n",
    "p1 = Person(\"James\", 40)\n",
    "p2 = Person(\"Alice\", 35)\n",
    "print(p1.name +\",\"+p2.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac6f8e",
   "metadata": {},
   "source": [
    "### Using Row class on PySpark RDD\n",
    "use Row class on PySpark RDD. When you use Row to create an RDD, after collecting the data you will get the result back in Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad66a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(name='James,,Smith', lang=['Java', 'Scala', 'C++'], state='CA'), Row(name='Michael,Rose,', lang=['Spark', 'Java', 'C++'], state='NJ'), Row(name='Robert,,Williams', lang=['CSharp', 'VB'], state='NV')]\n"
     ]
    }
   ],
   "source": [
    "data = [Row(name=\"James,,Smith\",lang=[\"Java\",\"Scala\",\"C++\"],state=\"CA\"), \n",
    "        Row(name=\"Michael,Rose,\",lang=[\"Spark\",\"Java\",\"C++\"],state=\"NJ\"),\n",
    "        Row(name=\"Robert,,Williams\",lang=[\"CSharp\",\"VB\"],state=\"NV\")]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833babe",
   "metadata": {},
   "source": [
    " collect the data and access the data using its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f753d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James,,Smith,['Java', 'Scala', 'C++']\n",
      "Michael,Rose,,['Spark', 'Java', 'C++']\n",
      "Robert,,Williams,['CSharp', 'VB']\n"
     ]
    }
   ],
   "source": [
    "collData=rdd.collect()\n",
    "for row in collData:\n",
    "    print(row.name + \",\" +str(row.lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994556a",
   "metadata": {},
   "source": [
    "Alternatively, you can also do by creating a Row like class “Person”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad80435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- lang: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df=spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39887a7",
   "metadata": {},
   "source": [
    "### Using Row class on PySpark DataFrame\n",
    "Row class also can be used with PySpark DataFrame, By default data in DataFrame represent as Row. To demonstrate, I will use the same data that was created for RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "555ddf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "df = spark.createDataFrame(data).toDF(*columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bda883",
   "metadata": {},
   "source": [
    "### Create Nested Struct Using Row Class\n",
    "way to create a struct type using the Row class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "559b09b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- prop: struct (nullable = true)\n",
      " |    |-- hair: string (nullable = true)\n",
      " |    |-- eye: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [Row(name=\"James\",prop=Row(hair=\"black\",eye=\"blue\")),\n",
    "        Row(name=\"Ann\",prop=Row(hair=\"grey\",eye=\"black\"))]\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc486e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
