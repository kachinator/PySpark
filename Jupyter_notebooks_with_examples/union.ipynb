{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union\n",
    "PySpark union() method of the DataFrame is used to merge two DataFrameâ€™s of the same structure/schema. If schemas are not the same it returns an error\n",
    "union() merges two datasets including duplicate records. PySpark recommends using DataFrame duplicate() function to remove duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/08 17:16:32 WARN Utils: Your hostname, javier-ubuntu, resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "25/08/08 17:16:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/08 17:16:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/08 17:16:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Union').getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "              (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "              (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "              (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Dataframe with the new records and some records from the above Dataframe but with the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData2 = [ (\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "                (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "                (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "                (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "                (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "            ]\n",
    "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge two or more DataFrames using union\n",
    "DataFrame union() method merges two DataFrames and returns the new DataFrame with all rows from two Dataframes regardless of duplicate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge without Duplicates\n",
    "Since the union() method returns all rows without `distinct` records, we will use the `distinct()` function to return just one record when duplicate exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===================================>                     (10 + 6) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "disDF = df.union(df2).distinct()\n",
    "disDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "disDF.select('employee_name','department','state','salary','age','bonus').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+\n",
      "|employee_name|department|state|salary|age|\n",
      "+-------------+----------+-----+------+---+\n",
      "|        James|     Sales|   NY| 90000| 34|\n",
      "|      Michael|     Sales|   NY| 86000| 56|\n",
      "|       Robert|     Sales|   CA| 81000| 30|\n",
      "|        Maria|   Finance|   CA| 90000| 24|\n",
      "|          Jen|   Finance|   NY| 79000| 53|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|\n",
      "+-------------+----------+-----+------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "disDF.select(disDF.columns[0:-1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "disDF.select(disDF.columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+\n",
      "|employee_name|department|state|salary|age|\n",
      "+-------------+----------+-----+------+---+\n",
      "|        James|     Sales|   NY| 90000| 34|\n",
      "|      Michael|     Sales|   NY| 86000| 56|\n",
      "|       Robert|     Sales|   CA| 81000| 30|\n",
      "|        Maria|   Finance|   CA| 90000| 24|\n",
      "|          Jen|   Finance|   NY| 79000| 53|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|\n",
      "+-------------+----------+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_small = disDF.select(disDF.columns[0:-1])\n",
    "df_small.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Merge Two DataFrames with Different Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 17:16:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+\n",
      "|   name|   dept|age|\n",
      "+-------+-------+---+\n",
      "|  James|  Sales| 34|\n",
      "|Michael|  Sales| 56|\n",
      "| Robert|  Sales| 30|\n",
      "|  Maria|Finance| 24|\n",
      "+-------+-------+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-----+---------+-----+------+\n",
      "| name|     dept|state|salary|\n",
      "+-----+---------+-----+------+\n",
      "|James|    Sales|   NY|  9000|\n",
      "|Maria|  Finance|   CA|  9000|\n",
      "|  Jen|  Finance|   NY|  7900|\n",
      "| Jeff|Marketing|   CA|  8000|\n",
      "+-----+---------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "#Create DataFrame df1 with columns name,dept & age\n",
    "data = [(\"James\",\"Sales\",34), (\"Michael\",\"Sales\",56), \\\n",
    "    (\"Robert\",\"Sales\",30), (\"Maria\",\"Finance\",24) ]\n",
    "columns= [\"name\",\"dept\",\"age\"]\n",
    "df1 = spark.createDataFrame(data = data, schema = columns)\n",
    "df1.printSchema()\n",
    "df1.show()\n",
    "\n",
    "#Create DataFrame df1 with columns name,dep,state & salary\n",
    "data2=[(\"James\",\"Sales\",\"NY\",9000),(\"Maria\",\"Finance\",\"CA\",9000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",7900),(\"Jeff\",\"Marketing\",\"CA\",8000)]\n",
    "columns2= [\"name\",\"dept\",\"state\",\"salary\"]\n",
    "df2 = spark.createDataFrame(data = data2, schema = columns2)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---+-----+------+\n",
      "|   name|   dept|age|state|salary|\n",
      "+-------+-------+---+-----+------+\n",
      "|  James|  Sales| 34| NULL|  NULL|\n",
      "|Michael|  Sales| 56| NULL|  NULL|\n",
      "| Robert|  Sales| 30| NULL|  NULL|\n",
      "|  Maria|Finance| 24| NULL|  NULL|\n",
      "+-------+-------+---+-----+------+\n",
      "\n",
      "+-----+---------+-----+------+----+\n",
      "| name|     dept|state|salary| age|\n",
      "+-----+---------+-----+------+----+\n",
      "|James|    Sales|   NY|  9000|NULL|\n",
      "|Maria|  Finance|   CA|  9000|NULL|\n",
      "|  Jen|  Finance|   NY|  7900|NULL|\n",
      "| Jeff|Marketing|   CA|  8000|NULL|\n",
      "+-----+---------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add missing columns 'state' & 'salary' to df1\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for column in [column for column in df2.columns if column not in df1.columns]:\n",
    "    df1 = df1.withColumn(column, lit(None))\n",
    "\n",
    "df1.show()\n",
    "\n",
    "#Add missing column 'age' to df2\n",
    "for column in [column for column in df1.columns if column not in df2.columns]:\n",
    "    df2 = df2.withColumn(column, lit(None))\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+-----+------+\n",
      "|   name|     dept| age|state|salary|\n",
      "+-------+---------+----+-----+------+\n",
      "|  James|    Sales|  34| NULL|  NULL|\n",
      "|Michael|    Sales|  56| NULL|  NULL|\n",
      "| Robert|    Sales|  30| NULL|  NULL|\n",
      "|  Maria|  Finance|  24| NULL|  NULL|\n",
      "|  James|    Sales|NULL|   NY|  9000|\n",
      "|  Maria|  Finance|NULL|   CA|  9000|\n",
      "|    Jen|  Finance|NULL|   NY|  7900|\n",
      "|   Jeff|Marketing|NULL|   CA|  8000|\n",
      "+-------+---------+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally join two dataframe's df1 & df2 by name\n",
    "merged_df=df1.unionByName(df2)\n",
    "merged_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
