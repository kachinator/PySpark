{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Distinct from DataFrame\n",
    "We can use `distinct().count()` of DataFrame or `countDistinct()` SQL function to get the count distinct.\n",
    "\n",
    "`distinct()` eliminates duplicate records(matching all columns of a Row) from DataFrame, `count()` returns the count of records on DataFrame. By chaining these you can get the count distinct of PySpark DataFrame.\n",
    "\n",
    "`countDistinct()` is a SQL function that could be used to get the count distinct of the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/08 15:27:08 WARN Utils: Your hostname, javier-ubuntu, resolves to a loopback address: 127.0.1.1; using 10.0.0.205 instead (on interface wlx0013eff3e14d)\n",
      "25/08/08 15:27:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/08 15:27:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   Name|     Dept|Salary|\n",
      "+-------+---------+------+\n",
      "|  James|    Sales|  3000|\n",
      "|Michael|    Sales|  4600|\n",
      "| Robert|    Sales|  4100|\n",
      "|  Maria|  Finance|  3000|\n",
      "|  James|    Sales|  3000|\n",
      "|  Scott|  Finance|  3300|\n",
      "|    Jen|  Finance|  3900|\n",
      "|   Jeff|Marketing|  3000|\n",
      "|  Kumar|Marketing|  2000|\n",
      "|   Saif|    Sales|  4100|\n",
      "+-------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "         .appName('SparkByExamples.com') \\\n",
    "         .getOrCreate()\n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "        (\"Michael\", \"Sales\", 4600),\n",
    "        (\"Robert\", \"Sales\", 4100),\n",
    "        (\"Maria\", \"Finance\", 3000),\n",
    "        (\"James\", \"Sales\", 3000),\n",
    "        (\"Scott\", \"Finance\", 3300),\n",
    "        (\"Jen\", \"Finance\", 3900),\n",
    "        (\"Jeff\", \"Marketing\", 3000),\n",
    "        (\"Kumar\", \"Marketing\", 2000),\n",
    "        (\"Saif\", \"Sales\", 4100)\n",
    "    ]\n",
    "    \n",
    "columns = [\"Name\",\"Dept\",\"Salary\"]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DataFrame distinct() and count()\n",
    "On the above DataFrame, we have a total of 10 rows and one row with all values duplicated, performing distinct count ( `distinct().count()` ) on this DataFrame should get us 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distinct Count: {str(df.distinct().count())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using countDistinct() SQL Function\n",
    "DataFrame `distinct()` returns a new DataFrame after eliminating duplicate rows (distinct on all columns). if you want to get count distinct on selected columns, use the PySpark SQL function `countDistinct()`. This function returns the number of distinct elements in a group.\n",
    "\n",
    "In order to use this function, you need to import it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|count(DISTINCT Dept, salary)|\n",
      "+----------------------------+\n",
      "|                           8|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "df2=df.select(countDistinct(\"Dept\", \"salary\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `countDistinct()` function returns a value in a Column type hence, you need to collect it to get the value from the DataFrame. And this function can be used to get the distinct count of any number of selected or all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of Dept & Salary: 8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Distinct Count of Dept & Salary: {str(df2.collect()[0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQL to get Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+\n",
      "|   Name|     Dept|Salary|\n",
      "+-------+---------+------+\n",
      "|  James|    Sales|  3000|\n",
      "|Michael|    Sales|  4600|\n",
      "| Robert|    Sales|  4100|\n",
      "|  Maria|  Finance|  3000|\n",
      "|  James|    Sales|  3000|\n",
      "|  Scott|  Finance|  3300|\n",
      "|    Jen|  Finance|  3900|\n",
      "|   Jeff|Marketing|  3000|\n",
      "|  Kumar|Marketing|  2000|\n",
      "|   Saif|    Sales|  4100|\n",
      "+-------+---------+------+\n",
      "\n",
      "+----------------------------------+\n",
      "|count(DISTINCT Name, Dept, Salary)|\n",
      "+----------------------------------+\n",
      "|                                 9|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "df.show()\n",
    "spark.sql(\"select count(distinct(*)) from EMP\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
