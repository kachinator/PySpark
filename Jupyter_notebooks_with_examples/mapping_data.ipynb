{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,Row\n",
    "from termcolor import cprint\n",
    "\n",
    "spark = SparkSession.builder.appName('mapping').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| level|\n",
      "+------+\n",
      "|Medium|\n",
      "|Medium|\n",
      "|Medium|\n",
      "|  High|\n",
      "|Medium|\n",
      "|Medium|\n",
      "|   Low|\n",
      "|   Low|\n",
      "|  High|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, map_from_arrays, array, col\n",
    "\n",
    "_dict = {\"High\":1, \"Medium\":2, \"Low\":3}\n",
    "\n",
    "df = spark.createDataFrame([[\"Medium\"], [\"Medium\"], [\"Medium\"], [\"High\"], [\"Medium\"], [\"Medium\"], [\"Low\"], [\"Low\"], [\"High\"]]\n",
    "                           , [\"level\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'High': 1, 'Medium': 2, 'Low': 3}\n"
     ]
    }
   ],
   "source": [
    "print(_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Column<''High''>, Column<''Medium''>, Column<''Low''>]\n"
     ]
    }
   ],
   "source": [
    "print(list(map(lit, _dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'map_from_arrays(array('High', 'Medium', 'Low'), array(1, 2, 3))[level]'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/.local/lib/python3.10/site-packages/pyspark/sql/classic/column.py:359: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "keys = array(list(map(lit, _dict.keys()))) # or alternatively [lit(k) for k in _dict.keys()]\n",
    "values = array(list(map(lit, _dict.values())))\n",
    "_map = map_from_arrays(keys, values)\n",
    "print( _map.getItem(col(\"level\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` and `map_from_arrays` to implement a key-based search mechanism for filling in the level_num field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "| level|level_num|\n",
      "+------+---------+\n",
      "|Medium|        2|\n",
      "|Medium|        2|\n",
      "|Medium|        2|\n",
      "|  High|        1|\n",
      "|Medium|        2|\n",
      "|Medium|        2|\n",
      "|   Low|        3|\n",
      "|   Low|        3|\n",
      "|  High|        1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keys = array(list(map(lit, _dict.keys()))) # or alternatively [lit(k) for k in _dict.keys()]\n",
    "values = array(list(map(lit, _dict.values())))\n",
    "#  Creates a new map from two arrays, map_from_arrays(col1, col2)  col1 keys. col2 values\n",
    "_map = map_from_arrays(keys, values)\n",
    "\n",
    "df_1 = df.withColumn(\"level_num\", _map.getItem(col(\"level\"))) # or element_at(_map, col(\"level\"))\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|     k|     v|\n",
      "+------+------+\n",
      "|[2, 5]|[a, b]|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             map|\n",
      "+----------------+\n",
      "|{2 -> a, 5 -> b}|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Creates a new map from two arrays, map_from_arrays(col1, col2)  col1 keys. col2 values\n",
    "df.select(map_from_arrays(df.k, df.v).alias(\"map\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Broadcast join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|map                  |\n",
      "+---------------------+\n",
      "|{1 -> Spain}         |\n",
      "|{2 -> Germany}       |\n",
      "|{3 -> Czech Republic}|\n",
      "|{4 -> Malta}         |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast, col, explode\n",
    "from pyspark.sql.types import IntegerType, MapType, StringType\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "# set up data\n",
    "map_df = spark.createDataFrame(\n",
    "    [({1: \"Spain\"},),({2: \"Germany\"},),({3: \"Czech Republic\"},),({4: \"Malta\"},)],\n",
    "    schema=StructType([StructField(\"map\", MapType(IntegerType(), StringType()))])\n",
    ")\n",
    "\n",
    "map_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|country_id|Sale|\n",
      "+----------+----+\n",
      "|         1| 200|\n",
      "|         2| 565|\n",
      "|         3| 467|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_df = spark.createDataFrame([(1, 200), (2, 565),(3,467)], [\"country_id\",\"Sale\"])\n",
    "sale_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|       country|Sale|\n",
      "+--------------+----+\n",
      "|         Spain| 200|\n",
      "|       Germany| 565|\n",
      "|Czech Republic| 467|\n",
      "+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "sale_df.join(\n",
    "    broadcast(map_df.select(explode(\"map\").alias(\"country_id\", \"country\"))), \n",
    "    on=\"country_id\",\n",
    "    how=\"left\"\n",
    ").select(\"country\", \"Sale\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead, you had your mapping as a single MapType, you could avoid the join by pushing the evaluation of the map up in execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, map_from_arrays, lit\n",
    "\n",
    "my_dict = {1: \"Spain\", 2: \"Germany\", 3: \"Czech Republic\", 4: \"Malta\"}\n",
    "my_map = map_from_arrays(\n",
    "    array(*map(lit, my_dict.keys())),\n",
    "    array(*map(lit, my_dict.values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|       country|Sale|\n",
      "+--------------+----+\n",
      "|         Spain| 200|\n",
      "|       Germany| 565|\n",
      "|Czech Republic| 467|\n",
      "+--------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_df.select(my_map.getItem(col(\"country_id\")).alias(\"country\"), \"Sale\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [map(keys: [1,2,3,4], values: [Spain,Germany,Czech Republic,Malta])[cast(country_id#176L as int)] AS country#202, Sale#177L]\n",
      "+- *(1) Scan ExistingRDD[country_id#176L,Sale#177L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_df.select(my_map.getItem(col(\"country_id\")).alias(\"country\"), \"Sale\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test mapping with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|company_name|country_of_source|\n",
      "+------------+-----------------+\n",
      "|         AEW|           Global|\n",
      "|      Apollo|           Global|\n",
      "|        Ares|           Global|\n",
      "|     Carlyle|           Global|\n",
      "|       CBREI|           Global|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Dataframe use for mapping\n",
    "data = [Row(company_name='AEW', country_of_source='Global'),\n",
    "        Row(company_name='Apollo', country_of_source='Global'),\n",
    "        Row(company_name='Ares', country_of_source='Global'),\n",
    "        Row(company_name='Carlyle', country_of_source='Global'),\n",
    "        Row(company_name='CBREI', country_of_source='Global'),\n",
    "]\n",
    "global_df =  spark.createDataFrame(data)\n",
    "global_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-----+\n",
      "|company_name|country_of_source|index|\n",
      "+------------+-----------------+-----+\n",
      "|      Apollo|           Canada|  3.8|\n",
      "|    JPMorgan|    United States|  4.8|\n",
      "|  Miysubishi|            Japan| 4.56|\n",
      "|        Ares|             NULL| 4.37|\n",
      "|     Carlyle|           Canada| NULL|\n",
      "|      Costco|             NULL| 3.98|\n",
      "+------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input test data\n",
    "in_data = [ Row(company_name='Apollo', country_of_source='Canada', index=3.80),\n",
    "            Row(company_name='JPMorgan', country_of_source='United States', index=4.80),\n",
    "            Row(company_name='Miysubishi', country_of_source='Japan', index=4.56),\n",
    "            Row(company_name='Ares', country_of_source=None, index=4.37),\n",
    "            Row(company_name='Carlyle', country_of_source='Canada', index=None),\n",
    "            Row(company_name='Costco', country_of_source=None, index=3.98)]\n",
    "in_df = spark.createDataFrame(in_data)\n",
    "in_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_df = global_df.toPandas().to_dict(orient='list')\n",
    "\n",
    "my_map = map_from_arrays(\n",
    "    array(*map(lit, dict_df['company_name'])),\n",
    "    array(*map(lit, dict_df['country_of_source']))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, map_from_arrays, lit\n",
    "\n",
    "dat_dict = {'AEW':'Global', 'Apollo':'Global', 'Ares':'Global', 'Carlyle':'Global', 'CBREI':'Global'}\n",
    "\n",
    "my_map = map_from_arrays(\n",
    "    array(*map(lit, dat_dict.keys())),\n",
    "    array(*map(lit, dat_dict.values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/.local/lib/python3.10/site-packages/pyspark/sql/classic/column.py:359: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------------+\n",
      "|country_of_source_x|company_name|index|country_of_source|\n",
      "+-------------------+------------+-----+-----------------+\n",
      "|             Global|      Apollo|  3.8|           Canada|\n",
      "|               NULL|    JPMorgan|  4.8|    United States|\n",
      "|               NULL|  Miysubishi| 4.56|            Japan|\n",
      "|             Global|        Ares| 4.37|             NULL|\n",
      "|             Global|     Carlyle| NULL|           Canada|\n",
      "|               NULL|      Costco| 3.98|             NULL|\n",
      "+-------------------+------------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_df.select(my_map.getItem(col(\"company_name\")).alias(\"country_of_source_x\"), \"company_name\", 'index', 'country_of_source').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------------+\n",
      "|country_of_source_x|company_name|index|country_of_source|\n",
      "+-------------------+------------+-----+-----------------+\n",
      "|             Global|      Apollo|  3.8|           Canada|\n",
      "|               NULL|    JPMorgan|  4.8|    United States|\n",
      "|               NULL|  Miysubishi| 4.56|            Japan|\n",
      "|             Global|        Ares| 4.37|             NULL|\n",
      "|             Global|     Carlyle| NULL|           Canada|\n",
      "|               NULL|      Costco| 3.98|             NULL|\n",
      "+-------------------+------------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_z = in_df.select(my_map.getItem(col(\"company_name\")).alias(\"country_of_source_x\"), \"company_name\", 'index', 'country_of_source')\n",
    "df_z.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------------+\n",
      "|company_name|index|country_of_source|\n",
      "+------------+-----+-----------------+\n",
      "|      Apollo|  3.8|           Global|\n",
      "|    JPMorgan|  4.8|    United States|\n",
      "|  Miysubishi| 4.56|            Japan|\n",
      "|        Ares| 4.37|           Global|\n",
      "|     Carlyle| NULL|           Global|\n",
      "|      Costco| 3.98|             NULL|\n",
      "+------------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_w = df_z.withColumn('country', coalesce(df_z['country_of_source_x'], df_z['country_of_source']))\n",
    "df_v = df_w.drop(\"country_of_source_x\", \"country_of_source\").withColumnRenamed('country', 'country_of_source')\n",
    "df_v.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----------------+\n",
      "|company_name|index|country_of_source|\n",
      "+------------+-----+-----------------+\n",
      "|      Apollo|  3.8|           Global|\n",
      "|    JPMorgan|  4.8|    United States|\n",
      "|  Miysubishi| 4.56|            Japan|\n",
      "|        Ares| 4.37|           Global|\n",
      "|     Carlyle| NULL|           Global|\n",
      "|      Costco| 3.98|             NULL|\n",
      "+------------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t2 = in_df.withColumn(\"country_of_source_updtd\", coalesce(my_map.getItem(col(\"company_name\")), in_df['country_of_source']))\\\n",
    "        .drop('country_of_source')\\\n",
    "        .withColumnRenamed('country_of_source_updtd', 'country_of_source')\n",
    "df_t2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+-----+\n",
      "|company_name|country_of_source|index|\n",
      "+------------+-----------------+-----+\n",
      "|      Apollo|           Global|  3.8|\n",
      "|    JPMorgan|    United States|  4.8|\n",
      "|  Miysubishi|            Japan| 4.56|\n",
      "|        Ares|           Global| 4.37|\n",
      "|     Carlyle|           Global| NULL|\n",
      "|      Costco|             NULL| 3.98|\n",
      "+------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_t3 = in_df.withColumn(\"country_of_source\", coalesce(my_map.getItem(col(\"company_name\")), in_df['country_of_source']))\n",
    "df_t3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
