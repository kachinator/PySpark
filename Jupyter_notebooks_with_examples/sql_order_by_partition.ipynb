{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/10 15:27:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+----------------+\n",
      "|loc_id|      date|          street|Surge_Protection|\n",
      "+------+----------+----------------+----------------+\n",
      "|    10|2024-11-23|     Lincoln Rd.|          0.5 Kw|\n",
      "|    25|2024-11-29|     High Street|          0.4 Kw|\n",
      "|    11|2023-10-18|Islington Square|          2.0 kW|\n",
      "|    89|2024-09-18|Leytonstone Road|          1.0 kW|\n",
      "|    25|2025-09-18|     London Ave.|          0.6 Kw|\n",
      "|    11|2024-10-18|  ST MARK'S HILL|         No Data|\n",
      "|    65|2023-02-26|        Main St.|          3.0 kW|\n",
      "|    11|2023-12-30|         Oak St.|         No Data|\n",
      "+------+----------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"order_by_partition\").getOrCreate()\n",
    "\n",
    "\n",
    "data = [('10', '2024-11-23','Lincoln Rd.','0.5 Kw'),\n",
    "        ('25', '2024-11-29','High Street','0.4 Kw'),\n",
    "        ('11', '2023-10-18','Islington Square','2.0 kW'),\n",
    "        ('89', '2024-09-18','Leytonstone Road','1.0 kW'),\n",
    "        ('25', '2025-09-18','London Ave.','0.6 Kw'),\n",
    "        ('11', '2024-10-18',\"ST MARK'S HILL\",'No Data'),\n",
    "        ('65', '2023-02-26','Main St.','3.0 kW'),\n",
    "        ('11', '2023-12-30','Oak St.','No Data')]\n",
    "\n",
    "df = spark.createDataFrame(data).toDF(\"loc_id\",'date','street','Surge_Protection')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('spc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "partition by `loc_id`  \n",
    "order by `date` DESC, `Surge_Protection` DESC  \n",
    "ROW_NUMBER() function is used to provide consecutive numbering of the rows  \n",
    "\n",
    "Is a window function that assigns a sequential integer to each row within the partition of a result set. The row number starts with 1 for the first row in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+----------------+---+\n",
      "|loc_id|date      |street          |Surge_Protection|rk |\n",
      "+------+----------+----------------+----------------+---+\n",
      "|10    |2024-11-23|Lincoln Rd.     |0.5 Kw          |1  |\n",
      "|11    |2024-10-18|ST MARK'S HILL  |No Data         |1  |\n",
      "|11    |2023-12-30|Oak St.         |No Data         |2  |\n",
      "|11    |2023-10-18|Islington Square|2.0 kW          |3  |\n",
      "|25    |2025-09-18|London Ave.     |0.6 Kw          |1  |\n",
      "|25    |2024-11-29|High Street     |0.4 Kw          |2  |\n",
      "|65    |2023-02-26|Main St.        |3.0 kW          |1  |\n",
      "|89    |2024-09-18|Leytonstone Road|1.0 kW          |1  |\n",
      "+------+----------+----------------+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql(\"\"\" SELECT *, ROW_NUMBER() OVER (PARTITION BY loc_id ORDER BY date DESC, Surge_Protection DESC) rk FROM spc\"\"\")\n",
    "        \n",
    "df1.show(truncate=False)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`partition by` clause divides the result set into partitions    \n",
    "ROW_NUMBER() function is applied to each partition separately and reinitialized the row number for each partition.   \n",
    "`order by` clause defines the logical order of the rows within each partition of the result set. The `order by` clause is mandatory because the ROW_NUMBER() function is order sensitive  \n",
    "rk choose the row taken from the partition, rk=1  => takes first row of each partititon            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+----------------+---+\n",
      "|loc_id|date      |street          |Surge_Protection|rk |\n",
      "+------+----------+----------------+----------------+---+\n",
      "|10    |2024-11-23|Lincoln Rd.     |0.5 Kw          |1  |\n",
      "|11    |2024-10-18|ST MARK'S HILL  |No Data         |1  |\n",
      "|25    |2025-09-18|London Ave.     |0.6 Kw          |1  |\n",
      "|65    |2023-02-26|Main St.        |3.0 kW          |1  |\n",
      "|89    |2024-09-18|Leytonstone Road|1.0 kW          |1  |\n",
      "+------+----------+----------------+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"\"\" select * \n",
    "                    from  (\n",
    "                        SELECT\n",
    "                        b.*, ROW_NUMBER() OVER (PARTITION BY loc_id ORDER BY date DESC, Surge_Protection DESC) rk\n",
    "                        FROM spc b\n",
    "                        ) WHERE rk = 1 \"\"\")\n",
    "\n",
    "df2.show(truncate=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
