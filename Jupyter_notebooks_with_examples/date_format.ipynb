{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('date-format').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+-------------------+------------+------------------+\n",
      "|current_date|yyyy MM dd|      MM/dd/yyyy|         MM/dd/yyyy|yyyy MMMM dd|    yyyy MMMM dd E|\n",
      "+------------+----------+----------------+-------------------+------------+------------------+\n",
      "|  2025-08-09|2025 08 09|08/09/2025 04:03|08/09/2025 04:03:34| 2025 Aug 09|2025 August 09 Sat|\n",
      "+------------+----------+----------------+-------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])\n",
    "df.select(current_date().alias(\"current_date\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MM dd\").alias(\"yyyy MM dd\"), \\\n",
    "      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm\").alias(\"MM/dd/yyyy\"), \\\n",
    "      date_format(current_timestamp(),\"MM/dd/yyyy hh:mm:SS\").alias(\"MM/dd/yyyy\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMM dd\").alias(\"yyyy MMMM dd\"), \\\n",
    "      date_format(current_timestamp(),\"yyyy MMMM dd E\").alias(\"yyyy MMMM dd E\") \\\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date=datetime.date(1997, 2, 28))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
    "df.select(to_date(df.t, format='yyyy-MM-dd HH:mm:ss').alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|            strt_tm|\n",
      "+-------------------+\n",
      "|12/11/2022 01:15:00|\n",
      "|2021-12-11 11:15:12|\n",
      "|05/12/2022 17:35:22|\n",
      "|07/12/2022 17:35:22|\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[start_time: timestamp, start_date: date]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    [(\"12/11/2022 01:15:00\",), (\"2021-12-11 11:15:12\",), (\"05/12/2022 17:35:22\",), (\"07/12/2022 17:35:22\",)], [\"strt_tm\"]\n",
    ").createOrReplaceTempView(\"table1\")\n",
    "\n",
    "spark.table(\"table1\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "select coalesce(\n",
    "            to_timestamp(strt_tm, 'dd/MM/yyyy HH:mm:ss'), \n",
    "            to_timestamp(strt_tm, 'yyyy-MM-dd HH:mm:ss')\n",
    "        ) as start_time,\n",
    "        coalesce(\n",
    "            to_date(strt_tm, 'dd/MM/yyyy HH:mm:ss'), \n",
    "            to_date(strt_tm, 'yyyy-MM-dd HH:mm:ss')\n",
    "        ) as start_date\n",
    "from table1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|to_timestamp(06-24-2019 12:01:19.000, MM-dd-yyyy HH:mm:ss.SSSS)|\n",
      "+---------------------------------------------------------------+\n",
      "|                                            2019-06-24 12:01:19|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('06-24-2019 12:01:19.000'),'MM-dd-yyyy HH:mm:ss.SSSS')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|to_timestamp(06/24/2019 12:01:19.000, MM/dd/yyyy HH:mm:ss.SSSS)|\n",
      "+---------------------------------------------------------------+\n",
      "|                                            2019-06-24 12:01:19|\n",
      "+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_timestamp(lit('06/24/2019 12:01:19.000'),'MM/dd/yyyy HH:mm:ss.SSSS')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading different date time formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+\n",
      "|Id      |TransactionDate    |UpdateDate         |\n",
      "+--------+-------------------+-------------------+\n",
      "|19994557|05/12/2022 17:35:22|08/10/2017 10:07:22|\n",
      "|19994557|04/08/2017 10:07:42|08/10/2017 12:37:04|\n",
      "+--------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = spark.createDataFrame(\n",
    "    [\n",
    "        (\"19994557\",\"05/12/2022 17:35:22\",\"08/10/2017 10:07:22\"),\n",
    "        (\"19994557\",\"04/08/2017 10:07:42\",\"08/10/2017 12:37:04\"),\n",
    "    ], \n",
    "     [\"Id\",\"TransactionDate\",\"UpdateDate\"]\n",
    ")\n",
    "\n",
    "t1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+-------------------+-------------------+\n",
      "|Id      |TransactionDate    |UpdateDate         |TransactionDate_2  |UpdateDate_new     |\n",
      "+--------+-------------------+-------------------+-------------------+-------------------+\n",
      "|19994557|05/12/2022 17:35:22|08/10/2017 10:07:22|2022-05-12 17:35:22|2017-08-10 10:07:22|\n",
      "|19994557|04/08/2017 10:07:42|08/10/2017 12:37:04|2017-04-08 10:07:42|2017-08-10 12:37:04|\n",
      "+--------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_t2 = t1.withColumn('TransactionDate_2',\n",
    "                      F.when(F.to_timestamp(F.col('TransactionDate'),'M/d/y H:m:s').isNotNull(),\n",
    "                             F.regexp_replace(F.to_timestamp(F.col('TransactionDate'),'M/d/y H:m:s'), r\"T\",\" \"))\n",
    "                      .otherwise(F.when(F.to_timestamp(F.col('TransactionDate'),'y-M-d H:m:s.SSS').isNotNull(),\n",
    "                                        F.regexp_replace(F.to_timestamp(F.col('TransactionDate'),'y-M-d H:m:s.SSS'), r\"T\",\" \"))\n",
    "                                .otherwise(F.when(F.to_timestamp(F.col('TransactionDate'),'y-M-d').isNotNull(),\n",
    "                                        F.regexp_replace(F.to_timestamp(F.col('TransactionDate'),'y-M-d'), r\"T\",\" \")))))                                        \n",
    "\n",
    "# df_t2.show(truncate=True)\n",
    "\n",
    "df_t3 = df_t2.withColumn('UpdateDate_new',\n",
    "                      F.when(F.to_timestamp(F.col('UpdateDate'),'M/d/y H:m:s').isNotNull(),\n",
    "                             F.regexp_replace(F.to_timestamp(F.col('UpdateDate'),'M/d/y H:m:s'), r\"T\",\" \"))\n",
    "                      .otherwise(F.when(F.to_timestamp(F.col('UpdateDate'),'y-M-d H:m:s.SSS').isNotNull(),\n",
    "                                        F.regexp_replace(F.to_timestamp(F.col('UpdateDate'),'y-M-d H:m:s.SSS'), r\"T\",\" \"))\n",
    "                                .otherwise(F.when(F.to_timestamp(F.col('UpdateDate'),'y-M-d').isNotNull(),\n",
    "                                        F.regexp_replace(F.to_timestamp(F.col('UpdateDate'),'y-M-d'), r\"T\",\" \")))))\n",
    "\n",
    "df_t3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
