{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/08 20:37:02 WARN Utils: Your hostname, javier-ubuntu, resolves to a loopback address: 127.0.1.1; using 172.17.0.1 instead (on interface docker0)\n",
      "25/08/08 20:37:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/javier/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/javier/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/javier/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7ca1a77e-b4a0-479b-a226-e45b01c7b883;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 154ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7ca1a77e-b4a0-479b-a226-e45b01c7b883\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/08/08 20:37:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/08 20:37:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "\n",
    "builder = SparkSession.builder.appName(\"transform\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame.transform()\n",
    "\n",
    "The `pyspark.sql.DataFrame.transform()` is used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.\n",
    "\n",
    "This function always returns the same number of rows that exists on the input PySpark DataFrame.\n",
    "\n",
    "DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame  \n",
    "    * func – Custom function to call.  \n",
    "    * *args – Arguments to pass to func.  \n",
    "    * *kwargs – Keyword arguments to pass to func.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/08 20:37:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+\n",
      "|CourseName|fee |discount|\n",
      "+----------+----+--------+\n",
      "|Java      |4000|5       |\n",
      "|Python    |4600|10      |\n",
      "|Scala     |4100|15      |\n",
      "|Scala     |4500|15      |\n",
      "|PHP       |3000|20      |\n",
      "+----------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "simpleData = (  (\"Java\",4000,5), \\\n",
    "                (\"Python\", 4600,10),  \\\n",
    "                (\"Scala\", 4100,15),   \\\n",
    "                (\"Scala\", 4500,15),   \\\n",
    "                (\"PHP\", 3000,20),  \\\n",
    "            )\n",
    "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`to_upper_str_columns()` – This function converts the CourseName column to upper case and updates the same column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformation 1\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"CourseName\",upper(df.CourseName))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduce_price()` – This function takes the argument and reduces the value from the fee and creates a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformation 2\n",
    "def reduce_price(reduceBy):\n",
    "    def inner(df):\n",
    "        return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
    "    return inner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`apply_discount()` – This creates a new column with the discounted fee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformation 3\n",
    "def apply_discount(df):\n",
    "    return df.withColumn(\"discounted_fee\",  df.new_fee - (df.new_fee * df.discount) / 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply DataFrame.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+--------+-------+--------------+\n",
      "|CourseName|fee |discount|new_fee|discounted_fee|\n",
      "+----------+----+--------+-------+--------------+\n",
      "|JAVA      |4000|5       |3000   |2850.0        |\n",
      "|PYTHON    |4600|10      |3600   |3240.0        |\n",
      "|SCALA     |4100|15      |3100   |2635.0        |\n",
      "|SCALA     |4500|15      |3500   |2975.0        |\n",
      "|PHP       |3000|20      |2000   |1600.0        |\n",
      "+----------+----+--------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.transform(to_upper_str_columns) \\\n",
    "        .transform(reduce_price(1000)) \\\n",
    "        .transform(apply_discount)        \n",
    "\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+---------------+\n",
      "|            Name|        Languages1|     Languages2|\n",
      "+----------------+------------------+---------------+\n",
      "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
      "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
      "+----------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame with Array\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    "        (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
    "        (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
    "        ]\n",
    "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax\n",
    "pyspark.sql.functions.transform(col, f)  \n",
    "The following are the parameters:\n",
    "\n",
    "    * col – ArrayType column\n",
    "    * f – Optional. Function to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|            Name|        languages1|\n",
      "+----------------+------------------+\n",
      "|    James,,Smith|[JAVA, SCALA, C++]|\n",
      "|   Michael,Rose,|[SPARK, JAVA, C++]|\n",
      "|Robert,,Williams|      [CSHARP, VB]|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using transform() function\n",
    "from pyspark.sql.functions import upper\n",
    "from pyspark.sql.functions import transform\n",
    "\n",
    "df.select('Name', transform(\"Languages1\", lambda x: upper(x)).alias(\"languages1\")).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------------------------------------+\n",
      "|id |ip             |temp                                            |\n",
      "+---+---------------+------------------------------------------------+\n",
      "|10 |68.28.91.22    |[35, 35, 35, 36, 35, 35, 32, 35, 30, 35, 32, 35]|\n",
      "|13 |67.185.72.1    |[45, 45, 45, 46, 45, 45, 42, 35, 40, 45, 42, 45]|\n",
      "|8  |208.109.163.218|[40, 40, 40, 40, 40, 43, 42, 40, 40, 45, 42, 45]|\n",
      "+---+---------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = [(10,\"68.28.91.22\",[35,35,35,36,35,35,32,35,30,35,32,35]),\n",
    "        (13,\"67.185.72.1\",[45,45,45,46,45,45,42,35,40,45,42,45]),\n",
    "        (8,\"208.109.163.218\",[40,40,40,40,40,43,42,40,40,45,42,45])\n",
    "        ]\n",
    "df = spark.createDataFrame(data=data,schema=[\"id\",\"ip\",\"temp\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------------------------------------------------------------------------+\n",
      "|id |ip             |temp_in F                                                                           |\n",
      "+---+---------------+------------------------------------------------------------------------------------+\n",
      "|10 |68.28.91.22    |[95.0, 95.0, 95.0, 96.8, 95.0, 95.0, 89.6, 95.0, 86.0, 95.0, 89.6, 95.0]            |\n",
      "|13 |67.185.72.1    |[113.0, 113.0, 113.0, 114.8, 113.0, 113.0, 107.6, 95.0, 104.0, 113.0, 107.6, 113.0] |\n",
      "|8  |208.109.163.218|[104.0, 104.0, 104.0, 104.0, 104.0, 109.4, 107.6, 104.0, 104.0, 113.0, 107.6, 113.0]|\n",
      "+---+---------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "df.select('id', 'ip', transform(\"temp\", lambda x: ((x * 9) / 5) + 32 ).alias(\"temp_in F\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------------------------------------------------------------------------+\n",
      "|id |ip             |fahrenheit_temp                                                                     |\n",
      "+---+---------------+------------------------------------------------------------------------------------+\n",
      "|10 |68.28.91.22    |[95.0, 95.0, 95.0, 96.8, 95.0, 95.0, 89.6, 95.0, 86.0, 95.0, 89.6, 95.0]            |\n",
      "|13 |67.185.72.1    |[113.0, 113.0, 113.0, 114.8, 113.0, 113.0, 107.6, 95.0, 104.0, 113.0, 107.6, 113.0] |\n",
      "|8  |208.109.163.218|[104.0, 104.0, 104.0, 104.0, 104.0, 109.4, 107.6, 104.0, 104.0, 113.0, 107.6, 113.0]|\n",
      "+---+---------------+------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql\n",
    "df.createOrReplaceTempView('df_view')\n",
    "spark.sql(\"\"\"select id, ip, \n",
    "             transform (temp, t -> ((t * 9) / 5) + 32 ) as fahrenheit_temp\n",
    "             from df_view\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+------------------------------------------------------------+\n",
      "|id |ip             |fahrenheit_temp                                             |\n",
      "+---+---------------+------------------------------------------------------------+\n",
      "|10 |68.28.91.22    |[95, 95, 95, 96, 95, 95, 89, 95, 86, 95, 89, 95]            |\n",
      "|13 |67.185.72.1    |[113, 113, 113, 114, 113, 113, 107, 95, 104, 113, 107, 113] |\n",
      "|8  |208.109.163.218|[104, 104, 104, 104, 104, 109, 107, 104, 104, 113, 107, 113]|\n",
      "+---+---------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('df_view')\n",
    "spark.sql(\"\"\"select id, ip, \n",
    "             transform (temp, t -> ((t * 9) div 5) + 32 ) as fahrenheit_temp\n",
    "             from df_view\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
